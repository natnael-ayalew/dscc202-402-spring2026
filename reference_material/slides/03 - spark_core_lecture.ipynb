{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Intensive Applications & Apache Spark Core\n",
    "\n",
    "## Run in GitHub Codespaces\n",
    "\n",
    "**Course**: DSCC 202-402 - Data Science at Scale  \n",
    "**Topics**: Big Data Fundamentals, Team Roles, Spark Architecture, DataFrames, Operations, & Advanced Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Sample Data for Demonstrations\n",
    "\n",
    "**Note**: Databricks notebooks have `spark` pre-initialized, so we don't need to create a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, count, when, max, min, expr, lit, datediff, current_date\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Core Lecture - Review\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get Spark Context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "ui_url = spark.sparkContext.uiWebUrl\n",
    "print(f\"Spark UI available at: {ui_url}\")\n",
    "print(\"üí° In GitHub Codespaces: Check the 'PORTS' tab below for forwarded port 4040 to access Spark UI\")\n",
    "\n",
    "# In Databricks, spark is already available\n",
    "# For local testing: spark = SparkSession.builder.appName(\"SparkCoreLecture\").getOrCreate()\n",
    "\n",
    "# Sample customer data\n",
    "customers_data = [\n",
    "    (\"C001\", \"Alice\", 25, \"CA\"), (\"C002\", \"Bob\", 30, \"NY\"),\n",
    "    (\"C003\", \"Carol\", 28, \"CA\"), (\"C004\", \"Dave\", 35, \"TX\"),\n",
    "    (\"C005\", \"Eve\", 22, \"NY\"), (\"C006\", \"Frank\", 45, \"CA\")\n",
    "]\n",
    "customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"name\", \"age\", \"state\"])\n",
    "\n",
    "# Sample transaction data\n",
    "transactions_data = [\n",
    "    (\"T001\", \"C001\", 100.0, \"Electronics\"), (\"T002\", \"C001\", 50.0, \"Books\"),\n",
    "    (\"T003\", \"C002\", 200.0, \"Electronics\"), (\"T004\", \"C003\", 150.0, \"Books\"),\n",
    "    (\"T005\", \"C002\", 75.0, \"Home\"), (\"T006\", \"C004\", 300.0, \"Electronics\"),\n",
    "    (\"T007\", \"C005\", 25.0, \"Books\"), (\"T008\", \"C001\", 120.0, \"Home\")\n",
    "]\n",
    "transactions_df = spark.createDataFrame(transactions_data, [\"txn_id\", \"customer_id\", \"amount\", \"category\"])\n",
    "\n",
    "print(\"‚úÖ Sample data created for demonstrations\")\n",
    "print(f\"   Customers: {customers_df.count()} rows\")\n",
    "print(f\"   Transactions: {transactions_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Course Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Data-Intensive Applications?\n",
    "\n",
    "### Key Characteristics:\n",
    "- Processes data that doesn't fit on a single machine\n",
    "- Uses distributed computing frameworks\n",
    "- Automates or augments decision-making\n",
    "- Requires specialized infrastructure and tools\n",
    "\n",
    "### Examples:\n",
    "- Recommendation systems (Netflix, Amazon)\n",
    "- Fraud detection (banks, credit cards)\n",
    "- Search engines (Google, Bing)\n",
    "- Real-time analytics dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals: Augment vs Replace Human Decision-Making\n",
    "\n",
    "### The Reality:\n",
    "- Data-intensive applications typically **augment** human capabilities\n",
    "- Humans provide context, judgment, and domain expertise\n",
    "- Systems provide scale, consistency, and data-driven insights\n",
    "- Best outcomes come from human-machine collaboration\n",
    "\n",
    "### Data Scale Considerations:\n",
    "- Data-intensive applications handle data that exceeds single-machine capacity\n",
    "- Require distributed processing across multiple machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Roadmap\n",
    "\n",
    "### Part 1: Foundations (Sections 1-4)\n",
    "1. Big Data fundamentals (5 V's, CRISP-DM)\n",
    "2. Team roles and collaboration\n",
    "3. Development best practices\n",
    "\n",
    "### Part 2: Spark Architecture (Sections 5-6)\n",
    "4. Cluster architecture (Driver, Executors)\n",
    "5. Core concepts (RDDs, lazy evaluation, immutability)\n",
    "\n",
    "### Part 3: Practical Skills (Sections 7-8)\n",
    "6. DataFrame operations\n",
    "7. Advanced topics (SQL, UDFs, caching)\n",
    "\n",
    "### Part 4: Integration (Section 9)\n",
    "8. Real-world examples and best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Big Data Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 4 V's of Big Data\n",
    "\n",
    "![4 V's of Big Data](../illustrations/4vs_of_big_data.png)\n",
    "\n",
    "### 1. **Volume** - The size of the data being processed\n",
    "- Terabytes, petabytes, exabytes of data\n",
    "- Example: Walmart processes 2.5 petabytes of data hourly\n",
    "\n",
    "### 2. **Velocity** - The speed at which data is generated and processed\n",
    "- Real-time or near real-time processing requirements\n",
    "- Example: Twitter handles 500 million tweets per day\n",
    "\n",
    "### 3. **Variety** - The different forms and sources of data\n",
    "- Structured (databases), semi-structured (JSON), unstructured (text, images)\n",
    "- Example: Social media data includes text, images, videos, metadata\n",
    "\n",
    "### 4. **Veracity** - The uncertainty or quality of the data\n",
    "- Data accuracy, trustworthiness, reliability\n",
    "- Example: Sensor data may have noise or missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP-DM: Cross-Industry Standard Process for Data Mining\n",
    "\n",
    "### The 6 Phases:\n",
    "![CRISP-DM](../illustrations/crispdm.png)\n",
    "\n",
    "1. **Business Understanding**: Define objectives and requirements\n",
    "2. **Data Understanding**: Collect and explore initial data\n",
    "3. **Data Preparation**: Clean, transform, and format data\n",
    "4. **Modeling**: Build and train models\n",
    "5. **Evaluation**: Assess model performance against business goals\n",
    "6. **Deployment**: Put model into production\n",
    "\n",
    "**Key Insight**: This is an iterative cycle, not a linear process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Considerations\n",
    "\n",
    "### When Do You Need Distributed Computing?\n",
    "\n",
    "### Indicators You Need Distributed Processing:\n",
    "- Data doesn't fit in memory on a single machine (> 16-32GB typical laptop RAM)\n",
    "- Processing time exceeds acceptable limits on single machine\n",
    "- Need for fault tolerance and high availability\n",
    "- Multiple data sources requiring parallel ingestion\n",
    "\n",
    "### Enter Apache Spark:\n",
    "- Processes terabytes to petabytes of data\n",
    "- Distributes computation across hundreds/thousands of machines\n",
    "- Provides fault tolerance through lineage tracking\n",
    "- Offers unified APIs for batch and streaming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Frequency/Impact Matrix\n",
    "\n",
    "### The Four Quadrants:\n",
    "\n",
    "| Impact \\ Frequency | **High Frequency** | **Low Frequency** |\n",
    "|-------------------|-------------------|------------------|\n",
    "| **High Impact** | üåü **Highest Value** <br> Many high-stakes decisions <br> Example: Automated trading | ‚ö†Ô∏è **Strategic Decisions** <br> Infrequent but critical <br> Example: Merger analysis |\n",
    "| **Low Impact** | ‚úÖ **Valuable Target** <br> Small individual impact, <br> large cumulative value <br> Example: Product recommendations | ‚ùå **Not Worth Building** <br> Low frequency, low impact <br> Example: One-off reports |\n",
    "\n",
    "### Key Insights:\n",
    "- **High Frequency/High Impact**: Maximum ROI for data applications\n",
    "- **High Frequency/Low Impact**: Still valuable due to cumulative effect\n",
    "- **Low Frequency/High Impact**: Less common, may need human oversight\n",
    "- **Low Frequency/Low Impact**: Generally not worth automation cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Data Team Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Five Key Roles in Data-Intensive Projects\n",
    "\n",
    "![Roles](../illustrations/roles.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role 1: Data Analyst\n",
    "\n",
    "### Key Activities:\n",
    "- Explore datasets to understand patterns and trends\n",
    "- Create visualizations and dashboards\n",
    "- Generate insights from data\n",
    "- Answer business questions with data\n",
    "- Communicate findings to stakeholders\n",
    "\n",
    "### Tools:\n",
    "- SQL, Python/R\n",
    "- Tableau, Power BI\n",
    "- Jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role 2: Data Engineer\n",
    "\n",
    "### Key Activities:\n",
    "- Design and implement data pipelines (ETL/ELT)\n",
    "- Manage data infrastructure\n",
    "- Ensure data quality and reliability\n",
    "- Optimize data storage and access\n",
    "- Handle data at scale\n",
    "\n",
    "### Tools:\n",
    "- Apache Spark, Airflow\n",
    "- Delta Lake, Databricks\n",
    "- Cloud platforms (AWS, Azure, GCP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role 3: Machine Learning Engineer\n",
    "\n",
    "### Key Activities:\n",
    "- Design and train ML models\n",
    "- Feature engineering\n",
    "- Model optimization and tuning\n",
    "- Deploy models to production\n",
    "- Monitor model performance\n",
    "\n",
    "### Tools:\n",
    "- Scikit-learn, TensorFlow, PyTorch\n",
    "- MLflow, Kubeflow\n",
    "- Model serving platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role 4: Evaluation Engineer\n",
    "\n",
    "### Key Activities:\n",
    "- Design evaluation metrics\n",
    "- Test model accuracy and reliability\n",
    "- Monitor models in production\n",
    "- Detect model drift and degradation\n",
    "- Ensure models meet business requirements\n",
    "\n",
    "### Tools:\n",
    "- A/B testing frameworks\n",
    "- Monitoring systems\n",
    "- Statistical analysis tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role 5: Domain Expert\n",
    "\n",
    "### Why Domain Experts Are Crucial:\n",
    "- Provide business context for data\n",
    "- Validate insights and models\n",
    "- Identify relevant features and patterns\n",
    "- Understand limitations and biases in data\n",
    "- Translate between technical and business stakeholders\n",
    "\n",
    "### Without Domain Experts:\n",
    "- ‚ùå Technical solutions may not address real business needs\n",
    "- ‚ùå Important patterns may be missed\n",
    "- ‚ùå Models may make unrealistic assumptions\n",
    "- ‚ùå Results may be misinterpreted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Collaboration Patterns\n",
    "\n",
    "### Effective Data Teams:\n",
    "- **Cross-functional**: All roles work together\n",
    "- **Iterative**: Follow CRISP-DM cycle\n",
    "- **Communicative**: Regular sync between roles\n",
    "- **Adaptable**: Respond to new insights and challenges\n",
    "\n",
    "### Typical Workflow:\n",
    "1. **Domain Expert** + **Data Analyst**: Define problem and explore data\n",
    "2. **Data Engineer**: Build pipelines to prepare data\n",
    "3. **ML Engineer**: Build and train models\n",
    "4. **Evaluation Engineer**: Test model performance\n",
    "5. **Data Engineer**: Deploy to production\n",
    "6. **All roles**: Monitor, iterate, and improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Development Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Pitfalls in Data-Intensive Applications\n",
    "\n",
    "![10 Ways Data Projects Fail](../illustrations/10_ways_data_projects_fail.jpeg)\n",
    "\n",
    "### Top Pitfalls to Avoid:\n",
    "1. **Lack of clear objectives** - Not defining success criteria\n",
    "2. **Insufficient data quality** - Garbage in, garbage out\n",
    "3. **Ignoring ethical considerations** - Bias, privacy, fairness\n",
    "4. **Poor team collaboration** - Silos between roles\n",
    "5. **Inadequate infrastructure** - Can't handle scale\n",
    "6. **No deployment plan** - Models never reach production\n",
    "7. **Lack of monitoring** - Models degrade over time\n",
    "8. **Overfitting** - Model doesn't generalize\n",
    "9. **Underestimating complexity** - Technical debt accumulates\n",
    "10. **Ignoring business context** - Solutions don't address real needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Best Practices\n",
    "\n",
    "### What Makes a Good Development Process:\n",
    "- ‚úÖ **Well-defined**: Clear stages and deliverables\n",
    "- ‚úÖ **Flexible**: Adapts to new insights and changes\n",
    "- ‚úÖ **Iterative**: Follows CRISP-DM cycle\n",
    "- ‚úÖ **Collaborative**: Involves all stakeholders\n",
    "\n",
    "### Other Best Practices:\n",
    "- Clear objectives and success metrics\n",
    "- Data quality checks and validation\n",
    "- Ethical considerations (bias, privacy, transparency)\n",
    "- Version control for code and data\n",
    "- Automated testing and monitoring\n",
    "- Documentation and knowledge sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Models Fail: Next Steps\n",
    "\n",
    "### Why More Data Is Often the Answer:\n",
    "- More training examples improve model generalization\n",
    "- Additional features may capture important patterns\n",
    "- More diverse data helps handle edge cases\n",
    "- Larger datasets reduce overfitting\n",
    "\n",
    "### The CRISP-DM Response to Failure:\n",
    "1. **Evaluate**: Understand why the model failed\n",
    "2. **Data Understanding**: Identify data gaps or quality issues\n",
    "3. **Data Preparation**: Acquire more data or improve existing data\n",
    "4. **Modeling**: Retrain with enhanced dataset\n",
    "5. **Evaluation**: Test again\n",
    "\n",
    "### Not Good First Steps:\n",
    "- ‚ùå Immediately change business goals (addresses symptoms, not cause)\n",
    "- ‚ùå Accept poor performance (defeats the purpose)\n",
    "- ‚ùå Abandon the project (too drastic without investigation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical Considerations and Data Quality\n",
    "\n",
    "### Ethical Principles:\n",
    "- **Fairness**: Avoid bias and discrimination\n",
    "- **Transparency**: Explain how decisions are made\n",
    "- **Privacy**: Protect sensitive information\n",
    "- **Accountability**: Take responsibility for outcomes\n",
    "\n",
    "### Data Quality Dimensions:\n",
    "- **Accuracy**: Data correctly represents reality\n",
    "- **Completeness**: No critical missing values\n",
    "- **Consistency**: Data is consistent across sources\n",
    "- **Timeliness**: Data is up-to-date\n",
    "- **Validity**: Data conforms to expected formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Success Factors Summary\n",
    "\n",
    "### To Build Successful Data-Intensive Applications:\n",
    "\n",
    "1. **Clear Vision**: Well-defined objectives aligned with business goals\n",
    "2. **Strong Team**: All 5 roles working collaboratively\n",
    "3. **Quality Data**: Accurate, complete, and relevant data\n",
    "4. **Robust Process**: Iterative development following CRISP-DM\n",
    "5. **Appropriate Technology**: Tools that scale (like Apache Spark!)\n",
    "6. **Ethical Framework**: Consider fairness, privacy, and accountability\n",
    "7. **Continuous Monitoring**: Track performance and iterate\n",
    "8. **Production Focus**: Plan for deployment from day one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Apache Spark Architecture\n",
    "\n",
    "Now we transition from general principles to Apache Spark specifics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "\n",
    "### Definition:\n",
    "Apache Spark is a **unified analytics engine for large-scale data processing**.\n",
    "\n",
    "### Key Features:\n",
    "- **Speed**: Up to 100x faster than Hadoop MapReduce\n",
    "- **Ease of Use**: High-level APIs in Python, Scala, Java, R\n",
    "- **Unified**: Supports batch processing, streaming, ML, graph processing\n",
    "- **Fault Tolerant**: Automatically recovers from failures\n",
    "- **Scalable**: Runs on single machine to thousands of nodes\n",
    "\n",
    "### When to Use Spark:\n",
    "- Processing terabytes to petabytes of data\n",
    "- Need for distributed computing\n",
    "- Complex data transformations and analytics\n",
    "- Machine learning at scale\n",
    "- Real-time streaming applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark API Hierarchy\n",
    "\n",
    "![Spark API](../illustrations/spark_api.png)\n",
    "\n",
    "### The API Stack (Bottom to Top):\n",
    "\n",
    "1. **RDD** (Resilient Distributed Dataset) - Foundational abstraction\n",
    "   - Low-level API\n",
    "   - Full control over data and operations\n",
    "   - All higher APIs build on RDDs\n",
    "\n",
    "2. **DataFrame** - Structured data with named columns\n",
    "   - High-level API\n",
    "   - Automatic optimizations (Catalyst)\n",
    "   - Similar to database tables or pandas DataFrames\n",
    "\n",
    "3. **Dataset** - Type-safe DataFrames (Scala/Java only)\n",
    "   - Compile-time type checking\n",
    "   - Object-oriented programming interface\n",
    "\n",
    "4. **Specialized APIs**:\n",
    "   - Spark SQL: SQL queries on structured data\n",
    "   - MLlib: Machine learning library\n",
    "   - GraphX: Graph processing\n",
    "   - Structured Streaming: Real-time stream processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Cluster Architecture\n",
    "\n",
    "![Spark Cluster](../illustrations/spark_cluster.png)\n",
    "\n",
    "### Components:\n",
    "\n",
    "1. **Driver Program**: Your application code\n",
    "2. **Cluster Manager**: Allocates resources (YARN, Mesos, Kubernetes, Standalone)\n",
    "3. **Executors**: Processes that run computations and store data\n",
    "\n",
    "### How They Work Together:\n",
    "1. Driver sends application to cluster manager\n",
    "2. Cluster manager allocates executors on worker nodes\n",
    "3. Driver sends tasks to executors\n",
    "4. Executors run tasks and return results to driver\n",
    "5. Driver coordinates the entire application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Driver Node\n",
    "\n",
    "### Driver Responsibilities:\n",
    "- Runs the `main()` function of your application\n",
    "- Creates the SparkContext/SparkSession\n",
    "- Converts user program into tasks\n",
    "- Schedules tasks on executors\n",
    "- Tracks executor status\n",
    "- Returns results to the user\n",
    "\n",
    "### In Databricks:\n",
    "- The notebook runs on the driver\n",
    "- SparkSession (`spark`) is pre-initialized\n",
    "- You interact with the driver when running cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Driver operations\n",
    "# In Databricks, 'spark' is already created (this is the driver's SparkSession)\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# The driver coordinates all operations\n",
    "print(\"\\nDriver is ready to execute your Spark code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executor Nodes\n",
    "\n",
    "### Executor Responsibilities:\n",
    "- Execute tasks sent by the driver\n",
    "- Store data for caching/persistence\n",
    "- Report computation results back to driver\n",
    "- Report metrics (memory usage, task duration)\n",
    "\n",
    "### Key Characteristics:\n",
    "- Multiple executors per application\n",
    "- Each executor runs in its own JVM process\n",
    "- Executors are long-lived (for the application duration)\n",
    "- Failed executors are automatically restarted\n",
    "\n",
    "### Executor vs Driver:\n",
    "- **Driver**: ONE per application, orchestrates everything\n",
    "- **Executors**: MANY per application, do the actual work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Hierarchy: Job, Stage, Task, Partition\n",
    "\n",
    "![Spark Execution](../illustrations/spark_execution.png)\n",
    "\n",
    "### The Four Levels:\n",
    "\n",
    "1. **Job** - A sequence of stages triggered by an action\n",
    "   - One job per action (count, collect, save)\n",
    "   - Example: `df.count()` triggers one job\n",
    "\n",
    "2. **Stage** - A set of tasks in a DAG\n",
    "   - Stages are divided at shuffle boundaries\n",
    "   - All tasks in a stage can run in parallel\n",
    "   - Example: Stage 1 (read + filter), Stage 2 (groupBy + aggregate)\n",
    "\n",
    "3. **Task** - A unit of work sent to an executor\n",
    "   - One task per partition\n",
    "   - Tasks are the smallest unit of execution\n",
    "   - Example: Process partition 1 of 200\n",
    "\n",
    "4. **Partition** - An atomic chunk of data (logical division) stored on a node\n",
    "   - Data is split into partitions for parallel processing\n",
    "   - Each partition is processed independently\n",
    "   - More partitions = more parallelism\n",
    "\n",
    "### Relationship:\n",
    "```\n",
    "Action ‚Üí Job ‚Üí Stages ‚Üí Tasks ‚Üí Partitions\n",
    "  1        1       N        M       M\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Understanding partitions\n",
    "\n",
    "print(\"Default partitions:\")\n",
    "print(f\"  Customers: {customers_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"  Transactions: {transactions_df.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Create a larger dataset to demonstrate partitioning\n",
    "large_df = spark.range(0, 1000000)\n",
    "print(f\"\\nLarge dataset: {large_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"Total rows: {large_df.count():,}\")\n",
    "print(f\"Rows per partition: ~{large_df.count() // large_df.rdd.getNumPartitions():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Deep Dive\n",
    "\n",
    "### RDD = Resilient Distributed Dataset\n",
    "\n",
    "**Resilient**: Fault-tolerant, automatically recovers from failures\n",
    "- Uses lineage information to recompute lost partitions\n",
    "\n",
    "**Distributed**: Data is split across multiple nodes\n",
    "- Enables parallel processing\n",
    "\n",
    "**Dataset**: Collection of objects\n",
    "- Can contain any type of data (rows, tuples, custom objects)\n",
    "\n",
    "### RDD Properties:\n",
    "- **Immutable**: Cannot be changed after creation\n",
    "- **Lazy**: Transformations are not computed until an action is called\n",
    "- **Partitioned**: Divided for parallel processing\n",
    "- **Typed**: Contains elements of a specific type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating and using RDDs\n",
    "\n",
    "# Create RDD from a Python list\n",
    "numbers_rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "print(\"RDD created from list:\")\n",
    "print(f\"  Type: {type(numbers_rdd)}\")\n",
    "print(f\"  Partitions: {numbers_rdd.getNumPartitions()}\")\n",
    "print(f\"  Elements: {numbers_rdd.collect()}\")\n",
    "\n",
    "# RDD transformations (lazy)\n",
    "squared_rdd = numbers_rdd.map(lambda x: x ** 2)\n",
    "even_rdd = squared_rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "print(\"\\nAfter transformations (map and filter):\")\n",
    "print(f\"  Even squares: {even_rdd.collect()}\")\n",
    "\n",
    "# Convert DataFrame to RDD\n",
    "customers_rdd = customers_df.rdd\n",
    "print(f\"\\nDataFrame to RDD:\")\n",
    "print(f\"  Type: {type(customers_rdd)}\")\n",
    "print(f\"  First row: {customers_rdd.first()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 6: Core Spark Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutability in Spark\n",
    "\n",
    "### What Does Immutability Mean?\n",
    "- Once created, a DataFrame **cannot be changed**\n",
    "- Transformations create **new** DataFrames\n",
    "- Original DataFrame remains unchanged\n",
    "\n",
    "### Why Immutability?\n",
    "1. **Fault Tolerance**: Can recreate data from lineage if node fails\n",
    "2. **Consistency**: Multiple operations see the same data\n",
    "3. **Optimization**: Engine can safely reorder operations\n",
    "4. **Parallelism**: Safe concurrent access without locks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Demonstrating immutability\n",
    "\n",
    "print(\"Original customers DataFrame:\")\n",
    "customers_df.show()\n",
    "\n",
    "# Apply transformation - creates NEW DataFrame\n",
    "customers_with_category = customers_df.withColumn(\n",
    "    \"age_category\",\n",
    "    when(col(\"age\") < 25, \"Young\")\n",
    "    .when(col(\"age\") < 35, \"Adult\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "\n",
    "print(\"\\nNew DataFrame with age_category:\")\n",
    "customers_with_category.show()\n",
    "\n",
    "print(\"\\nOriginal DataFrame is UNCHANGED (immutability):\")\n",
    "customers_df.show()\n",
    "\n",
    "print(f\"\\nOriginal DataFrame columns: {customers_df.columns}\")\n",
    "print(f\"New DataFrame columns: {customers_with_category.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Lineage and Fault Tolerance\n",
    "\n",
    "![RDD Lineage](../illustrations/rdd_lineage.png)\n",
    "\n",
    "### How Lineage Works:\n",
    "1. Spark tracks the **sequence of transformations** applied to data\n",
    "2. Creates a **Directed Acyclic Graph (DAG)** of dependencies\n",
    "3. If a partition is lost (node failure), Spark:\n",
    "   - Traces back through the lineage\n",
    "   - Recomputes only the lost partition\n",
    "   - Continues execution\n",
    "\n",
    "### Example Lineage:\n",
    "```\n",
    "Original Data ‚Üí filter() ‚Üí map() ‚Üí groupBy() ‚Üí Action\n",
    "```\n",
    "\n",
    "If partition fails during `groupBy()`, Spark recomputes:  \n",
    "`filter()` ‚Üí `map()` ‚Üí `groupBy()` for that partition only\n",
    "\n",
    "### Benefits:\n",
    "- No need for expensive data replication\n",
    "- Automatic recovery without user intervention\n",
    "- Efficient - only recomputes what's needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations vs Actions Overview\n",
    "\n",
    "### Placeholder Diagram: Transformations vs Actions Comparison Table\n",
    "\n",
    "| **Transformations (Lazy)** | **Actions (Eager)** |\n",
    "|---------------------------|---------------------|\n",
    "| Return new RDD/DataFrame | Return value to driver or write to storage |\n",
    "| Not computed immediately | Trigger immediate execution |\n",
    "| Build execution plan (DAG) | Execute the DAG |\n",
    "| Examples: | Examples: |\n",
    "| - `map()` | - `count()` |\n",
    "| - `filter()` | - `collect()` |\n",
    "| - `groupBy()` | - `show()` |\n",
    "| - `join()` | - `take()` |\n",
    "| - `select()` | - `first()` |\n",
    "| - `where()` | - `save()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations: Lazy Operations\n",
    "\n",
    "### Common DataFrame Transformations:\n",
    "\n",
    "1. **select()**: Choose specific columns\n",
    "2. **filter() / where()**: Filter rows by condition\n",
    "3. **withColumn()**: Add or modify columns\n",
    "4. **groupBy()**: Group data for aggregation\n",
    "5. **join()**: Combine DataFrames\n",
    "6. **union()**: Concatenate DataFrames\n",
    "7. **orderBy() / sort()**: Sort data\n",
    "8. **distinct()**: Remove duplicates\n",
    "\n",
    "### Key Characteristic:\n",
    "- **Nothing is computed** when you call a transformation\n",
    "- Spark just records the operation in the execution plan\n",
    "- Actual computation happens when an action is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Chaining transformations (all lazy)\n",
    "\n",
    "print(\"Defining transformation chain (no computation yet)...\\n\")\n",
    "\n",
    "# Chain of transformations\n",
    "result_df = transactions_df \\\n",
    "    .filter(col(\"amount\") > 50) \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(count(\"*\").alias(\"transaction_count\"),\n",
    "         sum(\"amount\").alias(\"total_amount\")) \\\n",
    "    .orderBy(col(\"total_amount\").desc())\n",
    "\n",
    "print(\"‚úÖ Transformations defined (not executed yet)\")\n",
    "print(f\"Result type: {type(result_df)}\")\n",
    "print(\"\\nNo actual data processing has occurred until we call an action!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions: Eager Operations\n",
    "\n",
    "### Common DataFrame Actions:\n",
    "\n",
    "1. **show()**: Display rows in console\n",
    "2. **count()**: Count number of rows\n",
    "3. **collect()**: Return all rows to driver\n",
    "4. **take(n)**: Return first n rows\n",
    "5. **first()**: Return first row\n",
    "6. **write()**: Save to storage\n",
    "7. **foreach()**: Apply function to each row\n",
    "\n",
    "### Key Characteristic:\n",
    "- **Triggers immediate execution** of all pending transformations\n",
    "- Returns results to the driver program\n",
    "- Can see output or write to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Actions trigger execution\n",
    "\n",
    "print(\"Now calling an ACTION - this triggers execution...\\n\")\n",
    "\n",
    "# This action triggers the entire transformation chain\n",
    "result_df.show()\n",
    "\n",
    "print(\"\\n‚úÖ Action completed - all transformations were executed!\")\n",
    "\n",
    "# Other actions\n",
    "print(f\"\\nNumber of categories: {result_df.count()}\")\n",
    "print(f\"First row: {result_df.first()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation Explained\n",
    "\n",
    "### How Lazy Evaluation Works:\n",
    "\n",
    "1. **User writes code** with transformations\n",
    "2. **Spark builds DAG** (Directed Acyclic Graph) of operations\n",
    "3. **No computation** happens yet\n",
    "4. **User calls action**\n",
    "5. **Spark optimizes** the DAG\n",
    "6. **Execution begins** and data is processed\n",
    "7. **Results returned** to user\n",
    "\n",
    "### Benefits:\n",
    "- **Optimization**: Spark can rearrange operations for efficiency\n",
    "- **Performance**: Avoids unnecessary intermediate results\n",
    "- **Resource Efficiency**: Only loads data that's needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Demonstrating lazy evaluation with timing\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LAZY EVALUATION DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a larger dataset for timing\n",
    "large_data = spark.range(0, 1000000)\n",
    "\n",
    "print(\"\\n1. Defining transformations (should be instant)...\")\n",
    "start = time.time()\n",
    "\n",
    "# Chain of transformations - all lazy\n",
    "transformed = large_data \\\n",
    "    .filter(col(\"id\") % 2 == 0) \\\n",
    "    .withColumn(\"squared\", col(\"id\") * col(\"id\")) \\\n",
    "    .filter(col(\"squared\") < 1000000)\n",
    "\n",
    "transform_time = time.time() - start\n",
    "print(f\"   Time to define transformations: {transform_time:.4f} seconds\")\n",
    "print(\"   ‚úì No computation occurred (lazy!)\")\n",
    "\n",
    "print(\"\\n2. Calling action (triggers execution)...\")\n",
    "start = time.time()\n",
    "\n",
    "# This action triggers all transformations\n",
    "result_count = transformed.count()\n",
    "\n",
    "action_time = time.time() - start\n",
    "print(f\"   Time to execute count(): {action_time:.4f} seconds\")\n",
    "print(f\"   Result: {result_count:,} rows\")\n",
    "print(\"   ‚úì All transformations executed!\")\n",
    "\n",
    "print(f\"\\nüìä Comparison:\")\n",
    "print(f\"   Transformation definition: {transform_time:.4f}s (instant - lazy)\")\n",
    "print(f\"   Action execution: {action_time:.4f}s (actual work - eager)\")\n",
    "print(f\"   Speed ratio: {action_time/transform_time:.0f}x slower for action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Lazy Evaluation\n",
    "\n",
    "### 1. Query Optimization\n",
    "Spark can analyze the entire chain and optimize:\n",
    "- **Predicate pushdown**: Move filters earlier\n",
    "- **Column pruning**: Only read needed columns\n",
    "- **Join reordering**: Optimize join sequence\n",
    "\n",
    "### 2. Avoiding Unnecessary Work\n",
    "```python\n",
    "df.filter(...).filter(...).take(10)  # Only processes enough data for 10 rows\n",
    "```\n",
    "\n",
    "### 3. Pipeline Optimization\n",
    "Multiple operations can be combined into single stage:\n",
    "```python\n",
    "df.select(...).filter(...).map(...)  # May execute in single pass\n",
    "```\n",
    "\n",
    "### 4. Fault Tolerance\n",
    "Lineage graph enables recomputation without storing intermediate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames vs RDDs\n",
    "\n",
    "### Why DataFrames Are Optimized:\n",
    "\n",
    "1. **Catalyst Optimizer**\n",
    "   - Analyzes and optimizes query plans\n",
    "   - Pushes filters and projections down\n",
    "   - Reorders operations for efficiency\n",
    "\n",
    "2. **Tungsten Execution Engine**\n",
    "   - Off-heap memory management\n",
    "   - Cache-aware computation\n",
    "   - Code generation at runtime\n",
    "\n",
    "3. **Schema Information**\n",
    "   - DataFrames have schema (column names and types)\n",
    "   - Enables better optimization decisions\n",
    "   - RDDs are opaque (Spark doesn't know structure)\n",
    "\n",
    "### When to Use What:\n",
    "- **DataFrames**: 95% of use cases (recommended)\n",
    "- **RDDs**: Complex custom logic, low-level control needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: DataFrame vs RDD performance\n",
    "\n",
    "print(\"Comparing DataFrame vs RDD operations:\\n\")\n",
    "\n",
    "# Create test data\n",
    "test_data = spark.range(0, 100000)\n",
    "\n",
    "# DataFrame approach (optimized)\n",
    "print(\"1. DataFrame approach (with Catalyst optimization):\")\n",
    "start = time.time()\n",
    "df_result = test_data \\\n",
    "    .filter(col(\"id\") % 2 == 0) \\\n",
    "    .filter(col(\"id\") < 50000) \\\n",
    "    .count()\n",
    "df_time = time.time() - start\n",
    "print(f\"   Result: {df_result:,} rows\")\n",
    "print(f\"   Time: {df_time:.4f} seconds\")\n",
    "\n",
    "# RDD approach (no optimization)\n",
    "print(\"\\n2. RDD approach (no Catalyst optimization):\")\n",
    "start = time.time()\n",
    "rdd_result = test_data.rdd \\\n",
    "    .filter(lambda row: row[0] % 2 == 0) \\\n",
    "    .filter(lambda row: row[0] < 50000) \\\n",
    "    .count()\n",
    "rdd_time = time.time() - start\n",
    "print(f\"   Result: {rdd_result:,} rows\")\n",
    "print(f\"   Time: {rdd_time:.4f} seconds\")\n",
    "\n",
    "print(f\"\\nüìä Performance:\")\n",
    "print(f\"   DataFrame: {df_time:.4f}s (optimized)\")\n",
    "print(f\"   RDD: {rdd_time:.4f}s (no optimization)\")\n",
    "if rdd_time > df_time:\n",
    "    print(f\"   DataFrame is {rdd_time/df_time:.1f}x faster! ‚úì\")\n",
    "\n",
    "print(\"\\nüí° DataFrames provide automatic optimizations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Partitions\n",
    "\n",
    "### Why Partitions Matter:\n",
    "- Each partition is processed by a **single task**\n",
    "- More partitions = more parallelism\n",
    "- But too many partitions = overhead\n",
    "\n",
    "### Rules of Thumb:\n",
    "- Aim for 2-4 partitions per CPU core\n",
    "- Partition size: 100MB - 1GB\n",
    "- For 8-core cluster: 16-32 partitions\n",
    "\n",
    "### Repartitioning:\n",
    "- **repartition(n)**: Increase or decrease partitions (full shuffle)\n",
    "- **coalesce(n)**: Decrease partitions (minimize shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Working with partitions\n",
    "\n",
    "# Check current partitions\n",
    "print(\"Current partitioning:\")\n",
    "print(f\"  Customers: {customers_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"  Transactions: {transactions_df.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Create data with specific number of partitions\n",
    "large_df = spark.range(0, 1000000, numPartitions=8)\n",
    "print(f\"\\nLarge dataset: {large_df.rdd.getNumPartitions()} partitions\")\n",
    "print(f\"Total rows: {large_df.count():,}\")\n",
    "\n",
    "# Repartition to increase parallelism\n",
    "repartitioned = large_df.repartition(16)\n",
    "print(f\"\\nAfter repartition(16): {repartitioned.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Coalesce to reduce partitions\n",
    "coalesced = repartitioned.coalesce(4)\n",
    "print(f\"After coalesce(4): {coalesced.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "print(\"\\nüí° More partitions = more parallel tasks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 7: DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 7: DataFrame Operations - Common Transformations\n\nThis section covers the most frequently used DataFrame operations. All the operations below have been demonstrated throughout this notebook with the sample customer and transaction data.\n\n### Key DataFrame Transformations:\n- **select()**: Choosing columns from DataFrames\n- **filter()/where()**: Filtering rows based on conditions\n- **withColumn()**: Adding/modifying columns\n- **withColumnRenamed()**: Renaming columns\n- **orderBy()**: Sorting DataFrames\n- **groupBy()**: Aggregating data\n- **union()**: Combining DataFrames with same schema\n- **join()**: Joining DataFrames on keys\n\n### Key DataFrame Actions:\n- **show()**: Displaying DataFrame contents (triggers execution)\n- **collect()**: Retrieving all rows to driver (triggers execution)\n- **count()**: Counting rows (triggers execution)\n\n---\n\n**Next**: We'll explore Advanced Topics including Spark SQL, UDFs, and Caching strategies."
  },
  {
   "cell_type": "markdown",
   "source": "## Section 8 Summary: Advanced Topics\n\n### What We Covered:\n\n1. **Spark SQL**\n   - Register DataFrames as temporary views with `createOrReplaceTempView()`\n   - Write SQL queries with `spark.sql(query)`\n   - Mix SQL and DataFrame API operations\n   - Same performance optimizations as DataFrame API\n\n2. **User-Defined Functions (UDFs)**\n   - **SQL UDFs**: `spark.udf.register('name', function, returnType)` for SQL queries\n   - **DataFrame API UDFs**: `udf()` decorator or function for DataFrame operations\n   - Enable custom business logic not available in built-in functions\n   - Note: Slightly less optimized than built-in functions\n\n3. **Caching**\n   - Use `.cache()` to store DataFrames in memory\n   - Cache **after expensive operations**, **before multiple uses**\n   - Dramatically improves performance for iterative workflows\n   - Remember to `.unpersist()` when done to free memory\n   - Best for: ML pipelines, interactive analysis, repeated queries\n\n### Key Takeaways:\n- Spark SQL provides familiar syntax for analysts\n- UDFs extend Spark with custom logic\n- Caching is essential for performance when reusing DataFrames\n- All three techniques are commonly used in production applications\n\n---\n\n**Next**: Final summary and resources",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example: Caching Performance Demonstration\n\nimport time\n\nprint(\"=\" * 60)\nprint(\"CACHING PERFORMANCE DEMONSTRATION\")\nprint(\"=\" * 60)\n\n# Create a larger dataset with more expensive transformations\nprint(\"\\nCreating a large dataset with expensive transformations...\")\n\n# Generate larger dataset - 10 million rows\nlarge_data = spark.range(0, 10000000) \\\n    .withColumn(\"value\", col(\"id\") * 2) \\\n    .withColumn(\"squared\", col(\"value\") * col(\"value\")) \\\n    .withColumn(\"category\", (col(\"id\") % 100).cast(StringType())) \\\n    .withColumn(\"subcategory\", (col(\"id\") % 1000).cast(StringType()))\n\n# Add expensive aggregation that will benefit from caching\nexpensive_df = large_data \\\n    .groupBy(\"category\") \\\n    .agg(\n        count(\"*\").alias(\"count\"),\n        sum(\"squared\").alias(\"sum_squared\"),\n        avg(\"value\").alias(\"avg_value\"),\n        max(\"squared\").alias(\"max_squared\"),\n        min(\"squared\").alias(\"min_squared\")\n    )\n\nprint(\"‚úÖ Complex transformation chain defined (lazy - not executed yet)\\n\")\n\n# Scenario 1: WITHOUT caching - Multiple actions on aggregated data\nprint(\"=\" * 60)\nprint(\"SCENARIO 1: WITHOUT CACHING\")\nprint(\"=\" * 60)\n\nprint(\"\\nPerforming 4 operations on the SAME aggregated dataset WITHOUT caching:\")\n\nstart = time.time()\ncount1 = expensive_df.count()\ntime1 = time.time() - start\nprint(f\"  Operation 1 (count categories): {count1} rows - {time1:.3f} seconds\")\n\nstart = time.time()\nmax_sum1 = expensive_df.agg(max(\"sum_squared\")).collect()[0][0]\ntime2 = time.time() - start\nprint(f\"  Operation 2 (max sum_squared): {max_sum1:,} - {time2:.3f} seconds\")\n\nstart = time.time()\navg_count1 = expensive_df.agg(avg(\"count\")).collect()[0][0]\ntime3 = time.time() - start\nprint(f\"  Operation 3 (avg count): {avg_count1:.1f} - {time3:.3f} seconds\")\n\nstart = time.time()\ntop_categories1 = expensive_df.orderBy(col(\"sum_squared\").desc()).take(5)\ntime4 = time.time() - start\nprint(f\"  Operation 4 (top 5 categories): retrieved - {time4:.3f} seconds\")\n\ntotal_time_no_cache = time1 + time2 + time3 + time4\nprint(f\"\\n  TOTAL TIME WITHOUT CACHE: {total_time_no_cache:.3f} seconds\")\nprint(\"  ‚ö†Ô∏è  Each operation recomputes the entire expensive aggregation!\")\n\n# Scenario 2: WITH caching - Multiple actions\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SCENARIO 2: WITH CACHING\")\nprint(\"=\" * 60)\n\nprint(\"\\nCaching the aggregated dataset before multiple operations...\")\ncached_data = expensive_df.cache()\n\n# First action: triggers computation AND caching\nprint(\"\\n  Performing first operation (triggers caching)...\")\nstart = time.time()\ncount2 = cached_data.count()\ntime_cache_load = time.time() - start\nprint(f\"  Operation 1 (count categories): {count2} rows - {time_cache_load:.3f} seconds\")\nprint(\"  ‚úì Data is now cached in memory\")\n\n# Subsequent actions: use cached data (should be much faster)\nprint(\"\\n  Performing subsequent operations (using cached data)...\")\n\nstart = time.time()\nmax_sum2 = cached_data.agg(max(\"sum_squared\")).collect()[0][0]\ntime_cached_2 = time.time() - start\nprint(f\"  Operation 2 (max sum_squared): {max_sum2:,} - {time_cached_2:.3f} seconds\")\n\nstart = time.time()\navg_count2 = cached_data.agg(avg(\"count\")).collect()[0][0]\ntime_cached_3 = time.time() - start\nprint(f\"  Operation 3 (avg count): {avg_count2:.1f} - {time_cached_3:.3f} seconds\")\n\nstart = time.time()\ntop_categories2 = cached_data.orderBy(col(\"sum_squared\").desc()).take(5)\ntime_cached_4 = time.time() - start\nprint(f\"  Operation 4 (top 5 categories): retrieved - {time_cached_4:.3f} seconds\")\n\ntotal_time_with_cache = time_cache_load + time_cached_2 + time_cached_3 + time_cached_4\nprint(f\"\\n  TOTAL TIME WITH CACHE: {total_time_with_cache:.3f} seconds\")\nprint(\"  ‚úì Operations 2-4 used cached data (much faster!)\")\n\n# Performance comparison\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PERFORMANCE COMPARISON\")\nprint(\"=\" * 60)\n\nspeedup = total_time_no_cache / total_time_with_cache\nprint(f\"\\nWithout caching: {total_time_no_cache:.3f} seconds\")\nprint(f\"With caching:    {total_time_with_cache:.3f} seconds\")\n\nif speedup > 1:\n    print(f\"Speedup:         {speedup:.2f}x faster with caching! ‚úì\")\n    print(f\"\\nBreakdown of cached operations:\")\n    print(f\"  Operation 2 speedup: {time2/time_cached_2:.2f}x faster\")\n    print(f\"  Operation 3 speedup: {time3/time_cached_3:.2f}x faster\")\n    print(f\"  Operation 4 speedup: {time4/time_cached_4:.2f}x faster\")\nelse:\n    print(f\"Note: Dataset may be small enough that Spark's optimization\")\n    print(f\"      makes recomputation competitive with caching overhead.\")\n    print(f\"      Caching shows more benefit with larger datasets and\")\n    print(f\"      more expensive transformations (joins, complex aggregations).\")\n\n# Best practice: Unpersist when done\ncached_data.unpersist()\nprint(\"\\n‚úÖ Cache cleared with unpersist() to free memory\")\n\n# Real-world example: Machine learning scenario\nprint(\"\\n\" + \"=\" * 60)\nprint(\"REAL-WORLD EXAMPLE: ML Feature Preparation\")\nprint(\"=\" * 60)\n\nprint(\"\\nScenario: Preparing features for multiple ML models\")\n\n# Expensive feature preparation - join is expensive!\nfeatures_df = transactions_df.alias(\"t\") \\\n    .join(customers_df.alias(\"c\"), \"customer_id\") \\\n    .groupBy(\"c.customer_id\", \"c.age\", \"c.state\") \\\n    .agg(\n        count(\"t.txn_id\").alias(\"txn_count\"),\n        sum(\"t.amount\").alias(\"total_spent\"),\n        avg(\"t.amount\").alias(\"avg_transaction\"),\n        max(\"t.amount\").alias(\"max_transaction\")\n    ) \\\n    .cache()  # Cache after expensive join and aggregation\n\nprint(\"‚úÖ Features prepared and cached\")\nprint(\"\\nNow you can use these features for multiple models:\")\nprint(\"  - Model 1: Predict customer churn\")\nprint(\"  - Model 2: Customer lifetime value prediction\")\nprint(\"  - Model 3: Segmentation analysis\")\nprint(\"\\nEach model uses the SAME cached features (no recomputation)!\")\n\n# Trigger caching\nfeatures_df.show(5)\n\n# Clean up\nfeatures_df.unpersist()\n\nprint(\"\\nüí° Key Takeaways:\")\nprint(\"   1. Cache DataFrames that are used MULTIPLE times\")\nprint(\"   2. Cache AFTER expensive operations (joins, aggregations)\")\nprint(\"   3. Caching benefits are most visible with:\")\nprint(\"      - Large datasets\")\nprint(\"      - Expensive transformations (joins, complex aggregations)\")\nprint(\"      - Iterative algorithms (ML training)\")\nprint(\"      - Interactive analysis with repeated queries\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Caching: Performance Optimization\n\n### What Is Caching?\nCaching stores a DataFrame in memory (or disk) so it doesn't need to be recomputed when used multiple times.\n\n### When to Use Cache:\n‚úÖ **GOOD Use Cases:**\n- DataFrame used multiple times in your workflow\n- After expensive transformations (joins, aggregations)\n- Iterative algorithms (machine learning)\n- Interactive analysis where you query the same data repeatedly\n\n‚ùå **BAD Use Cases:**\n- DataFrame only used once\n- Very large DataFrames that don't fit in memory\n- Simple transformations that are cheap to recompute\n\n### How to Cache:\n```python\ndf.cache()  # or df.persist()\n```\n\n### Best Practice:\nCache **after** expensive operations, **before** multiple uses.\n\n### Memory Considerations:\n- Cached data uses cluster memory\n- Can evict old cached data if memory is full (LRU)\n- Use `unpersist()` to free memory when done",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example: User-Defined Functions (UDFs)\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType, DoubleType, IntegerType\n\nprint(\"=\" * 60)\nprint(\"USER-DEFINED FUNCTIONS (UDF) DEMONSTRATION\")\nprint(\"=\" * 60)\n\n# Register DataFrames as temporary views (needed for SQL UDFs)\ncustomers_df.createOrReplaceTempView(\"customers\")\ntransactions_df.createOrReplaceTempView(\"transactions\")\n\n# Method 1: SQL UDF Registration\nprint(\"\\n1. SQL UDF - Register for use in SQL queries\")\nprint(\"-\" * 60)\n\n# Define and register a UDF for age category\ndef categorize_age(age):\n    \"\"\"Categorize customers by age group\"\"\"\n    if age < 25:\n        return \"Young Adult\"\n    elif age < 35:\n        return \"Adult\"\n    elif age < 50:\n        return \"Middle Age\"\n    else:\n        return \"Senior\"\n\n# Register the UDF for SQL\nspark.udf.register(\"categorize_age\", categorize_age, StringType())\nprint(\"‚úÖ Registered 'categorize_age' UDF for SQL\")\n\n# Define and register a UDF for discount calculation\ndef calculate_discount(amount):\n    \"\"\"Calculate discount based on purchase amount\"\"\"\n    if amount >= 200:\n        return amount * 0.15  # 15% discount\n    elif amount >= 100:\n        return amount * 0.10  # 10% discount\n    else:\n        return amount * 0.05  # 5% discount\n\nspark.udf.register(\"calculate_discount\", calculate_discount, DoubleType())\nprint(\"‚úÖ Registered 'calculate_discount' UDF for SQL\\n\")\n\n# Use UDFs in SQL query\nquery = \"\"\"\n    SELECT \n        name,\n        age,\n        categorize_age(age) as age_category,\n        SUM(amount) as total_spent,\n        calculate_discount(SUM(amount)) as total_discount\n    FROM customers c\n    JOIN transactions t ON c.customer_id = t.customer_id\n    GROUP BY c.customer_id, name, age\n    ORDER BY total_spent DESC\n\"\"\"\n\nprint(\"SQL Query with UDFs:\")\nresult = spark.sql(query)\nresult.show()\n\n# Method 2: DataFrame API UDF\nprint(\"\\n2. DataFrame API UDF - Use with withColumn()\")\nprint(\"-\" * 60)\n\n# Define UDF for DataFrame API\n@udf(returnType=StringType())\ndef format_customer_label(name, state):\n    \"\"\"Create a formatted customer label\"\"\"\n    return f\"{name} ({state})\"\n\n# Alternative syntax without decorator\nspending_tier_udf = udf(\n    lambda amount: \"High Spender\" if amount > 150 else \"Regular\" if amount > 75 else \"Low Spender\",\n    StringType()\n)\n\nprint(\"‚úÖ Created UDFs for DataFrame API\\n\")\n\n# Apply UDFs to DataFrame\ncustomer_summary = customers_df.alias(\"c\") \\\n    .join(transactions_df.alias(\"t\"), \"customer_id\") \\\n    .groupBy(\"c.customer_id\", \"c.name\", \"c.state\") \\\n    .agg(sum(\"t.amount\").alias(\"total_spent\")) \\\n    .withColumn(\"customer_label\", format_customer_label(col(\"name\"), col(\"state\"))) \\\n    .withColumn(\"spending_tier\", spending_tier_udf(col(\"total_spent\")))\n\nprint(\"DataFrame with UDF results:\")\ncustomer_summary.select(\"customer_label\", \"total_spent\", \"spending_tier\").show(truncate=False)\n\n# Example 3: More complex UDF\nprint(\"\\n3. Complex UDF Example - Multiple inputs\")\nprint(\"-\" * 60)\n\ndef calculate_loyalty_score(transaction_count, total_spent, avg_amount):\n    \"\"\"\n    Calculate customer loyalty score based on multiple factors\n    Score ranges from 0-100\n    \"\"\"\n    # Base score from transaction frequency (max 40 points)\n    frequency_score = transaction_count * 10\n    if frequency_score > 40:\n        frequency_score = 40\n    \n    # Volume score from total spending (max 40 points)\n    volume_score = total_spent / 10\n    if volume_score > 40:\n        volume_score = 40\n    \n    # Consistency score from average transaction (max 20 points)\n    consistency_score = avg_amount / 10\n    if consistency_score > 20:\n        consistency_score = 20\n    \n    return int(frequency_score + volume_score + consistency_score)\n\n# Register for SQL\nspark.udf.register(\"loyalty_score\", calculate_loyalty_score, IntegerType())\n\n# Use in SQL query\nloyalty_query = \"\"\"\n    SELECT \n        c.name,\n        COUNT(t.txn_id) as txn_count,\n        SUM(t.amount) as total_spent,\n        AVG(t.amount) as avg_amount,\n        loyalty_score(COUNT(t.txn_id), SUM(t.amount), AVG(t.amount)) as loyalty_score\n    FROM customers c\n    JOIN transactions t ON c.customer_id = t.customer_id\n    GROUP BY c.customer_id, c.name\n    ORDER BY loyalty_score DESC\n\"\"\"\n\nprint(\"Customer Loyalty Scores:\")\nspark.sql(loyalty_query).show()\n\nprint(\"\\nüí° UDFs enable custom business logic in Spark transformations!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## User-Defined Functions (UDFs)\n\n### What Are UDFs?\nUser-Defined Functions allow you to define custom logic that can be applied to DataFrame columns.\n\n### Two Ways to Use UDFs:\n\n1. **SQL UDFs**: Register with `spark.udf.register()` for use in SQL queries\n2. **DataFrame API UDFs**: Use `udf()` decorator/function for DataFrame operations\n\n### When to Use UDFs:\n- Custom business logic not available in built-in functions\n- Complex string parsing or transformations\n- Domain-specific calculations\n- Integration with external libraries\n\n### Important Notes:\n- UDFs are less optimized than built-in functions\n- Use built-in functions when possible\n- UDFs serialize/deserialize data (performance overhead)\n- Good for clarity even with slight performance cost",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example: Using Spark SQL\n\nprint(\"=\" * 60)\nprint(\"SPARK SQL DEMONSTRATION\")\nprint(\"=\" * 60)\n\n# Step 1: Register DataFrames as temporary views\ncustomers_df.createOrReplaceTempView(\"customers\")\ntransactions_df.createOrReplaceTempView(\"transactions\")\n\nprint(\"\\n‚úÖ Registered DataFrames as SQL views: 'customers' and 'transactions'\\n\")\n\n# Step 2: Run SQL queries\nprint(\"Query 1: Customer transaction summary\")\nquery1 = \"\"\"\n    SELECT \n        c.customer_id,\n        c.name,\n        c.state,\n        COUNT(t.txn_id) as transaction_count,\n        SUM(t.amount) as total_spent,\n        AVG(t.amount) as avg_transaction\n    FROM customers c\n    LEFT JOIN transactions t ON c.customer_id = t.customer_id\n    GROUP BY c.customer_id, c.name, c.state\n    ORDER BY total_spent DESC\n\"\"\"\n\nresult1 = spark.sql(query1)\nresult1.show()\n\n# Query 2: Category performance by state\nprint(\"\\nQuery 2: Category performance by state\")\nquery2 = \"\"\"\n    SELECT \n        c.state,\n        t.category,\n        COUNT(*) as num_transactions,\n        SUM(t.amount) as revenue\n    FROM customers c\n    JOIN transactions t ON c.customer_id = t.customer_id\n    GROUP BY c.state, t.category\n    ORDER BY c.state, revenue DESC\n\"\"\"\n\nresult2 = spark.sql(query2)\nresult2.show()\n\n# Query 3: High-value customers (spending > $200)\nprint(\"\\nQuery 3: High-value customers\")\nquery3 = \"\"\"\n    SELECT \n        c.name,\n        c.age,\n        SUM(t.amount) as total_spent\n    FROM customers c\n    JOIN transactions t ON c.customer_id = t.customer_id\n    GROUP BY c.customer_id, c.name, c.age\n    HAVING SUM(t.amount) > 200\n    ORDER BY total_spent DESC\n\"\"\"\n\nresult3 = spark.sql(query3)\nresult3.show()\n\nprint(\"\\nüí° Spark SQL provides familiar syntax with DataFrame performance!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Spark SQL: Querying DataFrames with SQL\n\n### Why Use Spark SQL?\n- Familiar SQL syntax for data analysts\n- Can mix SQL and DataFrame API operations\n- Same optimizations as DataFrame API (Catalyst)\n- Great for ad-hoc analysis and exploration\n\n### How It Works:\n1. Register DataFrame as a temporary view\n2. Run SQL queries against the view\n3. Results returned as DataFrames\n\n### Benefits:\n- Leverage existing SQL knowledge\n- Easy integration with BI tools\n- Readable queries for complex logic",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Section 8: Advanced Topics\n\nNow let's explore advanced Spark features that are essential for real-world applications!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary\n",
    "\n",
    "This comprehensive lecture deck has covered all core Spark concepts:\n",
    "\n",
    "### üéØ Core Learning Objectives Achieved:\n",
    "\n",
    "1. **Data-Intensive Applications Foundations**\n",
    "   - 5 V's of Big Data and CRISP-DM methodology\n",
    "   - Team roles and collaboration patterns\n",
    "   - Development best practices and pitfalls\n",
    "   - Decision-making frameworks\n",
    "\n",
    "2. **Apache Spark Architecture**\n",
    "   - Driver and Executor roles\n",
    "   - Job, Stage, Task, and Partition hierarchy\n",
    "   - RDD as the fundamental data structure\n",
    "   - Fault tolerance through lineage\n",
    "\n",
    "3. **Core Spark Concepts**\n",
    "   - Immutability principles\n",
    "   - Lazy evaluation and optimization\n",
    "   - Transformations vs Actions\n",
    "   - DataFrames vs RDDs\n",
    "\n",
    "4. **Practical DataFrame Operations**\n",
    "   - Selection, filtering, and projection\n",
    "   - Column operations and schema modification\n",
    "   - Aggregations and grouping\n",
    "   - Joins and unions\n",
    "   - Actions that trigger execution\n",
    "\n",
    "5. **Advanced Techniques**\n",
    "   - Spark SQL integration\n",
    "   - User-Defined Functions (UDFs)\n",
    "   - Strategic caching for performance\n",
    "   - End-to-end pipeline design\n",
    "\n",
    "### üìö Study Recommendations:\n",
    "\n",
    "- **Review this notebook** section by section\n",
    "- **Run all code examples** in Databricks to see outputs\n",
    "- **Practice with the labs** to reinforce concepts\n",
    "- **Understand the 'why'** behind each concept, not just memorize syntax\n",
    "\n",
    "### üöÄ Ready for Success!\n",
    "\n",
    "You now have a complete reference with executable examples,  \n",
    "conceptual explanations, and best practices for Apache Spark.\n",
    "\n",
    "---\n",
    "\n",
    "**Format**: üìì Jupyter Notebook with markdown and code cells  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}