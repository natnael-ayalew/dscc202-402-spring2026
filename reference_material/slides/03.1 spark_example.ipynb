{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame Example: Word Count on Shakespeare\n",
    "\n",
    "This notebook demonstrates how to use Spark DataFrames to process text data through a complete data transformation pipeline.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By working through this example, you will:\n",
    "- Understand how DataFrames provide a structured approach to data processing\n",
    "- Learn the declarative \"what not how\" programming paradigm\n",
    "- See how the Catalyst optimizer enhances performance\n",
    "- Understand Spark execution plans and two-phase aggregation\n",
    "- Practice transforming unstructured data into structured insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We'll Build\n",
    "\n",
    "We'll create a complete data processing pipeline:\n",
    "\n",
    "1. **Load**: Read unstructured text (Shakespeare's complete works)\n",
    "2. **Transform**: Convert raw text into structured DataFrame\n",
    "3. **Clean**: Normalize words (lowercase, remove punctuation)\n",
    "4. **Aggregate**: Count word frequencies\n",
    "5. **Analyze**: Sort and visualize results\n",
    "\n",
    "This pattern applies to any data processing task at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "**Declarative Programming**: With DataFrames, you specify *what* you want, not *how* to compute it. Spark's Catalyst optimizer figures out the most efficient execution strategy.\n",
    "\n",
    "**Lazy Evaluation**: Transformations are not executed immediately. Spark builds an execution plan and only runs it when you call an action (like `.collect()` or `.show()`).\n",
    "\n",
    "**Catalyst Optimizer**: Spark's query optimizer that applies intelligent transformations like predicate pushdown, column pruning, and two-phase aggregation to make your code run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    explode, split, lower, col, regexp_replace, length\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Shakespeare text\n",
    "FILEPATH = \"shakespeare.txt\"\n",
    "URL = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
    "\n",
    "if not Path(FILEPATH).exists():\n",
    "    print(f\"Downloading from {URL}...\")\n",
    "    urllib.request.urlretrieve(URL, FILEPATH)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "file_size = Path(FILEPATH).stat().st_size\n",
    "print(f\"File size: {file_size / 1024:.1f} KB ({file_size / 1024 / 1024:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark_DataFrame_Word_Count_Example\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Running on: {sc.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Spark Session Initialization\n",
    "\n",
    "**Note for Databricks users**: In Databricks notebooks, the Spark session is automatically created and available as the `spark` variable. You don't need to create it explicitly.\n",
    "\n",
    "**This example runs in a codespace environment** where explicit session creation is required using `SparkSession.builder`. When moving between local development and cloud platforms like Databricks, remember this key difference:\n",
    "\n",
    "- **Local/Codespace**: Must create session with `SparkSession.builder.getOrCreate()`\n",
    "- **Databricks**: Session pre-created, just use the `spark` variable\n",
    "\n",
    "This is one of the conveniences of managed Spark environments like Databricks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "We're working with the **Complete Works of William Shakespeare** from MIT OpenCourseWare. This dataset includes:\n",
    "\n",
    "- All of Shakespeare's plays (tragedies, comedies, histories)\n",
    "- All sonnets and poems\n",
    "- Approximately 5.3 MB of text\n",
    "- Over 900,000 words total\n",
    "\n",
    "This is a great dataset for word frequency analysis because:\n",
    "- Large enough to see Spark's distributed processing\n",
    "- Real-world literary text with rich vocabulary\n",
    "- Publicly available and well-known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Declarative Approach\n",
    "\n",
    "With DataFrames, you write code that describes **what** you want to achieve, not **how** to implement it step-by-step.\n",
    "\n",
    "Think of it like SQL: you specify the transformations you want, and Spark's Catalyst optimizer figures out the most efficient way to execute them.\n",
    "\n",
    "**Benefits**:\n",
    "- More readable and maintainable code\n",
    "- Automatic performance optimizations\n",
    "- Language-independent (same code pattern in Python, Scala, Java)\n",
    "- Catalyst can reorder operations, push down filters, and use two-phase aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_dataframe(filepath: str):\n",
    "    \"\"\"\n",
    "    DataFrame word count - declarative style.\n",
    "    \n",
    "    We specify WHAT we want:\n",
    "    - Split lines into words\n",
    "    - Normalize to lowercase and remove punctuation\n",
    "    - Count occurrences\n",
    "    - Sort by frequency\n",
    "    \n",
    "    Catalyst optimizer figures out HOW to execute efficiently.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read.text(filepath)\n",
    "        # Split into words and explode to rows\n",
    "        .select(explode(split(col(\"value\"), r\"\\s+\")).alias(\"word\"))\n",
    "        # Normalize: lowercase, keep only letters\n",
    "        .select(regexp_replace(lower(col(\"word\")), r\"[^a-z]\", \"\").alias(\"word\"))\n",
    "        # Filter empty strings\n",
    "        .filter(length(col(\"word\")) > 0)\n",
    "        # Group and count\n",
    "        .groupBy(\"word\").count()\n",
    "        # Sort by frequency descending\n",
    "        .orderBy(col(\"count\").desc())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Transformation Breakdown\n",
    "\n",
    "Let's understand each transformation in our pipeline:\n",
    "\n",
    "1. **`spark.read.text(filepath)`**: Loads the text file as a DataFrame with one column called `value`. Each line becomes one row.\n",
    "\n",
    "2. **`explode(split(col(\"value\"), r\"\\s+\"))`**: Splits each line on whitespace into an array, then explodes the array so each word becomes its own row. One line with 10 words → 10 rows.\n",
    "\n",
    "3. **`regexp_replace(lower(col(\"word\")), r\"[^a-z]\", \"\")`**: Converts to lowercase and removes all non-letter characters (punctuation, numbers). \"Hello!\" → \"hello\"\n",
    "\n",
    "4. **`filter(length(col(\"word\")) > 0)`**: Removes empty strings that resulted from pure punctuation.\n",
    "\n",
    "5. **`groupBy(\"word\").count()`**: Groups identical words together and counts occurrences. This triggers a **shuffle** - data is redistributed across partitions.\n",
    "\n",
    "6. **`orderBy(col(\"count\").desc())`**: Sorts by count in descending order to show most frequent words first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute and display results\n",
    "start = time.perf_counter()\n",
    "df_result = word_count_dataframe(FILEPATH).collect()\n",
    "execution_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Execution time: {execution_time:.3f}s\\n\")\n",
    "print(\"Top 20 words in Shakespeare:\")\n",
    "print(\"=\" * 40)\n",
    "for i, row in enumerate(df_result[:20], 1):\n",
    "    print(f\"{i:2d}. {row['word']:15} {row['count']:>6,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "**Declarative code**: The function reads like a description of the transformation, not implementation details. You can understand what it does without knowing how Spark executes it.\n",
    "\n",
    "**Schema awareness**: Each transformation understands column types. The `count()` aggregation knows it's working with strings grouped by key.\n",
    "\n",
    "**Lazy evaluation**: The transformation chain isn't executed until `.collect()` is called. Spark uses this time to analyze and optimize the entire plan.\n",
    "\n",
    "**Performance**: Despite running locally, this processes ~900,000 words efficiently. The same code scales to terabytes of data on a cluster.\n",
    "\n",
    "**Common words dominate**: English follows Zipf's law - a few function words (\"the\", \"and\", \"of\") account for a large percentage of all text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Understanding Spark Execution Plans\n",
    "\n",
    "One of Spark's most powerful features is **query optimization**. When you write DataFrame code, Spark doesn't execute it directly. Instead:\n",
    "\n",
    "1. **Logical Plan**: Spark converts your code into a logical plan of operations\n",
    "2. **Optimized Plan**: The Catalyst optimizer applies transformations (predicate pushdown, constant folding, etc.)\n",
    "3. **Physical Plan**: Spark generates the actual execution strategy\n",
    "\n",
    "The **physical execution plan** shows exactly how Spark will execute your query. Let's examine it!\n",
    "\n",
    "**How to read execution plans**: Read **bottom-up**. Data flows from step (1) at the bottom upward to the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display execution plan\n",
    "df_query = word_count_dataframe(FILEPATH)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHYSICAL EXECUTION PLAN\")\n",
    "print(\"=\" * 70)\n",
    "df_query.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Plan Walkthrough\n",
    "\n",
    "Let's break down what each step means (reading bottom-up):\n",
    "\n",
    "#### **Step 1: Scan text**\n",
    "Reads the raw text file. Each line becomes one row with a single column called `value`. The `ReadSchema` confirms Spark only reads what we need (column pruning optimization).\n",
    "\n",
    "#### **Step 2: Generate** \n",
    "The `explode(split(...))` operation. Splits each line on whitespace and \"explodes\" the resulting array so each word becomes its own row. One line with 10 words → 10 rows.\n",
    "\n",
    "#### **Step 3: Filter**\n",
    "Removes empty strings. Notice the condition combines `lower()`, `regexp_replace()`, and `length()` - **Catalyst fused these operations** into a single pass rather than doing three separate iterations. This is predicate fusion!\n",
    "\n",
    "#### **Step 4: Project**\n",
    "Applies the lowercase + regex cleanup transformation to produce normalized words.\n",
    "\n",
    "#### **Step 5: HashAggregate (partial)**\n",
    "This is the first half of **two-phase aggregation**. Each partition computes local counts *before* shuffling. Instead of shuffling every word occurrence, Spark shuffles pre-aggregated `(word, partial_count)` pairs. Major optimization!\n",
    "\n",
    "#### **Step 6: Exchange** ⚠️ **SHUFFLE**\n",
    "Data is redistributed across the cluster using `hashpartitioning(word, 8)`. All occurrences of the same word end up on the same partition. Shuffles involve network I/O and are expensive, which is why Spark minimizes them.\n",
    "\n",
    "#### **Step 7: HashAggregate (final)**\n",
    "Combines the partial counts from all partitions into final counts per word.\n",
    "\n",
    "#### **Step 8: Exchange (for sorting)**\n",
    "Another shuffle using `rangepartitioning` to prepare for global sorting. Data is distributed so partition 1 has the highest counts, partition 2 the next highest, etc.\n",
    "\n",
    "#### **Step 9: Sort**\n",
    "Sorts within each partition. Because of range partitioning, the final result is globally sorted.\n",
    "\n",
    "#### **Step 10: AdaptiveSparkPlan**\n",
    "Wrapper indicating **Adaptive Query Execution (AQE)** is enabled. Spark can modify this plan at runtime based on actual data statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts from the Execution Plan\n",
    "\n",
    "| Concept | What It Means | Why It Matters |\n",
    "|---------|---------------|----------------|\n",
    "| **Two-Phase Aggregation** | Partial aggregation → shuffle → final aggregation | Reduces data movement by pre-aggregating locally |\n",
    "| **Exchange (Shuffle)** | Data redistribution across partitions | Expensive operation; necessary for correct grouping |\n",
    "| **HashPartitioning** | `hash(key) % num_partitions` determines partition | Ensures all same keys go to same partition |\n",
    "| **Predicate Pushdown** | Filters applied early in the pipeline | Reduces data processed in later stages |\n",
    "| **AdaptiveSparkPlan** | Runtime optimization enabled | Spark adjusts plan based on actual data characteristics |\n",
    "\n",
    "**The Big Picture**: Catalyst automatically applied optimizations we didn't ask for! The two-phase aggregation and predicate fusion make the code significantly faster than a naive implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 20 words\n",
    "top_words = df_result[:20]\n",
    "words = [row[\"word\"] for row in top_words]\n",
    "counts = [row[\"count\"] for row in top_words]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "y_pos = np.arange(len(words))\n",
    "plt.barh(y_pos, counts, color='#3498db', edgecolor='black', linewidth=0.5)\n",
    "plt.yticks(y_pos, words, fontsize=11)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Frequency\", fontsize=12, fontweight='bold')\n",
    "plt.ylabel(\"Word\", fontsize=12, fontweight='bold')\n",
    "plt.title(\"Top 20 Words in Shakespeare's Complete Works\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add count labels\n",
    "for i, count in enumerate(counts):\n",
    "    plt.text(count + 100, i, f\"{count:,}\", va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('shakespeare_top_words.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to: shakespeare_top_words.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency distribution (Zipf's law)\n",
    "all_counts = [row[\"count\"] for row in df_result]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear scale\n",
    "ax1 = axes[0]\n",
    "ax1.hist(all_counts, bins=50, color='#e74c3c', edgecolor='black', alpha=0.7, linewidth=0.5)\n",
    "ax1.set_xlabel(\"Word Frequency\", fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel(\"Number of Unique Words\", fontsize=11, fontweight='bold')\n",
    "ax1.set_title(\"Distribution of Word Frequencies\", fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale\n",
    "ax2 = axes[1]\n",
    "ax2.hist(all_counts, bins=50, color='#e74c3c', edgecolor='black', alpha=0.7, linewidth=0.5)\n",
    "ax2.set_xlabel(\"Word Frequency (log scale)\", fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel(\"Number of Unique Words (log scale)\", fontsize=11, fontweight='bold')\n",
    "ax2.set_title(\"Distribution (Log Scale) - Power Law\", fontsize=12, fontweight='bold')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('shakespeare_frequency_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to: shakespeare_frequency_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Insights: Zipf's Law\n",
    "\n",
    "The frequency distribution reveals **Zipf's Law**, a fundamental pattern in natural language:\n",
    "\n",
    "**Observations**:\n",
    "- **Most words are rare**: The vast majority of words appear only a few times\n",
    "- **Few words dominate**: A small set of words (\"the\", \"and\", \"of\") account for a large percentage of total text\n",
    "- **Power law distribution**: When plotted on log-log scale, forms a roughly straight line\n",
    "\n",
    "**Zipf's Law states**: The frequency of a word is inversely proportional to its rank. The 2nd most common word appears about half as often as the 1st, the 3rd about one-third as often, etc.\n",
    "\n",
    "**Why this matters**:\n",
    "- Shows that language is highly compressible (few words carry most information)\n",
    "- Explains why word frequency is a good feature for machine learning\n",
    "- Common in many real-world datasets (not just text): city populations, website traffic, income distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. Declarative Approach\n",
    "Write **what** you want, let Spark handle **how**. This makes code more readable and maintainable.\n",
    "\n",
    "### 2. Efficient Execution\n",
    "Catalyst optimizer applies intelligent transformations like two-phase aggregation and predicate pushdown automatically.\n",
    "\n",
    "### 3. Lazy Evaluation\n",
    "Transformations don't execute until an action is called. This allows Spark to optimize the entire pipeline before running anything.\n",
    "\n",
    "### 4. Scalability\n",
    "The same code works on gigabytes or terabytes of data. Spark handles partitioning and distribution automatically.\n",
    "\n",
    "### 5. Readable & Expressive\n",
    "DataFrame API with SQL-like syntax is intuitive for data analysts and engineers. The execution plan provides visibility into how Spark runs your code.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: Explore more complex transformations like joins, window functions, and custom aggregations. The declarative pattern scales to much more sophisticated analytics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n",
    "print(\"\\nGreat job! You've completed a full Spark DataFrame transformation pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
