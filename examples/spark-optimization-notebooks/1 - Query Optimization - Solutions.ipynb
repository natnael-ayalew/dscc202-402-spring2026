{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Query Optimization - Solutions\n",
    "\n",
    "**Objective**: Master Spark query optimization techniques to write efficient data processing pipelines.\n",
    "\n",
    "**Learning Outcomes**:\n",
    "- Understand how Catalyst optimizer consolidates filters\n",
    "- Recognize when caching helps vs hurts performance\n",
    "- Identify predicate pushdown in explain plans\n",
    "- Compare columnar (Parquet) vs row-based (CSV) formats\n",
    "- Apply optimization principles to avoid common anti-patterns\n",
    "\n",
    "**Estimated Time**: 45 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nimport time\n\nspark = SparkSession.builder \\\n    .appName(\"Lab-Query-Optimization\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.logLevel\", \"ERROR\") \\\n    .getOrCreate()\n\nsc = spark.sparkContext\nsc.setLogLevel(\"ERROR\")\n\nprint(f\"üöÄ Spark {spark.version} - Query Optimization Lab\")\nui_url = spark.sparkContext.uiWebUrl\nprint(f\"Spark UI: {ui_url}\")\nprint(\"üí° In GitHub Codespaces: Check the 'PORTS' tab below for forwarded port 4040 to access Spark UI\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load IoT Sensor Data\n",
    "\n",
    "We'll use IoT sensor readings to demonstrate query optimization techniques. This dataset contains sensor measurements from multiple buildings with various sensor types (temperature, humidity, motion, pressure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data for initial examples\n",
    "df = spark.read.csv(\"../Datasets/iot_sensor_readings.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"üìä IoT Sensor Dataset:\")\n",
    "print(f\"Total records: {df.count():,}\")\n",
    "print(f\"\\nSchema:\")\n",
    "df.printSchema()\n",
    "print(f\"\\nSample data:\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Logical Optimizations\n",
    "\n",
    "Spark's **Catalyst optimizer** automatically optimizes query plans. Let's explore how it handles multiple filter transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.1: Multiple Chained Filters\n",
    "\n",
    "It's natural to write filters separately for readability. Let's see what Catalyst does with multiple chained filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple chained filters - might seem inefficient\n",
    "limit_sensors_df = (df\n",
    "    .filter(col(\"sensor_id\") != \"TEMP_001\")\n",
    "    .filter(col(\"sensor_id\") != \"TEMP_002\") \n",
    "    .filter(col(\"sensor_id\") != \"HUMID_001\")\n",
    "    .filter(col(\"sensor_id\") != \"HUMID_002\")\n",
    "    .filter(col(\"sensor_id\") != \"MOTION_001\")\n",
    "    .filter(col(\"location\") != \"Building_A_Floor_1\")\n",
    "    .filter(col(\"location\") != \"Building_A_Floor_2\")\n",
    "    .filter(col(\"location\") != \"Building_B_Floor_1\")\n",
    ")\n",
    "\n",
    "print(\"Multiple chained filters - Explain plan:\")\n",
    "print(\"=\"*80)\n",
    "limit_sensors_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observation**: Notice in the **Optimized Logical Plan** that Catalyst automatically consolidates all the filters into a single Filter operation with combined conditions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.2: Single Consolidated Filter\n",
    "\n",
    "We could write it ourselves with a single filter. Let's compare the plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single consolidated filter - manually optimized\n",
    "better_df = (df\n",
    "    .filter(\n",
    "        (col(\"sensor_id\").isNotNull()) &\n",
    "        (col(\"sensor_id\") != \"TEMP_001\") &\n",
    "        (col(\"sensor_id\") != \"TEMP_002\") &\n",
    "        (col(\"sensor_id\") != \"HUMID_001\") &\n",
    "        (col(\"sensor_id\") != \"HUMID_002\") &\n",
    "        (col(\"sensor_id\") != \"MOTION_001\") &\n",
    "        (col(\"location\") != \"Building_A_Floor_1\") &\n",
    "        (col(\"location\") != \"Building_A_Floor_2\") &\n",
    "        (col(\"location\") != \"Building_B_Floor_1\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Single consolidated filter - Explain plan:\")\n",
    "print(\"=\"*80)\n",
    "better_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observation**: The **Optimized Logical Plan** is nearly identical! Catalyst produces the same optimized query regardless of how you write it. Write for readability - Catalyst handles optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1.3: Duplicate Filters\n",
    "\n",
    "In complex queries, you might accidentally duplicate filter conditions. Let's see how Catalyst handles this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate filters - accidentally filtering same condition multiple times\n",
    "duplicate_df = (df\n",
    "    .filter(col(\"status\") != \"ERROR\")\n",
    "    .filter(col(\"status\") != \"ERROR\")  # Duplicate\n",
    "    .filter(col(\"status\") != \"ERROR\")  # Duplicate\n",
    "    .filter(col(\"status\") != \"ERROR\")  # Duplicate\n",
    "    .filter(col(\"status\") != \"ERROR\")  # Duplicate\n",
    ")\n",
    "\n",
    "print(\"Duplicate filters - Explain plan:\")\n",
    "print(\"=\"*80)\n",
    "duplicate_df.explain(True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Result: Catalyst eliminates duplicates and creates a single filter!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Filter Optimization Analysis\n",
    "\n",
    "**Task**: Create two DataFrames:\n",
    "1. One with 5 chained filters on different columns\n",
    "2. One with the same conditions combined in a single filter\n",
    "\n",
    "Use `explain()` to verify the optimized plans are identical, then confirm both produce the same count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Filter Optimization Exercise\n",
    "print(\"üéØ Exercise 1.1: Filter Optimization Analysis\\n\")\n",
    "\n",
    "# Approach 1: Chained filters\n",
    "exercise_chained = (df\n",
    "    .filter(col(\"status\") == \"NORMAL\")\n",
    "    .filter(col(\"unit\") == \"celsius\")\n",
    "    .filter(col(\"value\") > 5)\n",
    "    .filter(col(\"value\") < 15)\n",
    "    .filter(col(\"battery_level\") > 70)\n",
    ")\n",
    "\n",
    "# Approach 2: Consolidated filter\n",
    "exercise_consolidated = df.filter(\n",
    "    (col(\"status\") == \"NORMAL\") &\n",
    "    (col(\"unit\") == \"celsius\") &\n",
    "    (col(\"value\") > 5) &\n",
    "    (col(\"value\") < 15) &\n",
    "    (col(\"battery_level\") > 70)\n",
    ")\n",
    "\n",
    "# Show explain plans\n",
    "print(\"Chained filters explain plan:\")\n",
    "exercise_chained.explain()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Consolidated filter explain plan:\")\n",
    "exercise_consolidated.explain()\n",
    "\n",
    "# Verify same results\n",
    "chained_count = exercise_chained.count()\n",
    "consolidated_count = exercise_consolidated.count()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Chained approach: {chained_count:,} records\")\n",
    "print(f\"Consolidated approach: {consolidated_count:,} records\")\n",
    "\n",
    "assert chained_count == consolidated_count, \"Counts should match!\"\n",
    "print(\"\\n‚úì Exercise 1.1 completed! Both approaches produce identical optimized plans and results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Caching\n",
    "\n",
    "By default, DataFrame data exists on a Spark cluster only while being processed during a query. You can explicitly persist a DataFrame using the **`cache()`** method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Cache (Best Practices)\n",
    "\n",
    "‚úÖ **DO cache when**:\n",
    "- **Exploratory data analysis**: Running multiple different queries on the same dataset\n",
    "- **Machine learning**: Iteratively training models on the same data\n",
    "- **Iterative algorithms**: Reusing the same DataFrame multiple times\n",
    "\n",
    "‚ùå **DON'T cache when**:\n",
    "- Using data only once (no benefit, wastes resources)\n",
    "- Caching consumes cluster memory that could be used for task execution\n",
    "- **Caching can prevent query optimizations** (like predicate pushdown, as we'll see next)\n",
    "\n",
    "‚ö†Ô∏è **Important**: Always call **`unpersist()`** when done with cached data to free up memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: When caching makes sense\n",
    "print(\"Example: Multiple operations on cached data\\n\")\n",
    "\n",
    "# Cache for iterative analysis\n",
    "analysis_df = df.filter(col(\"status\") == \"NORMAL\").cache()\n",
    "\n",
    "# First action triggers caching\n",
    "print(f\"Total NORMAL readings: {analysis_df.count():,}\")\n",
    "\n",
    "# Subsequent operations use cached data (faster)\n",
    "print(f\"Average value: {analysis_df.agg({'value': 'avg'}).collect()[0][0]:.2f}\")\n",
    "print(f\"Distinct sensors: {analysis_df.select('sensor_id').distinct().count()}\")\n",
    "print(f\"Distinct locations: {analysis_df.select('location').distinct().count()}\")\n",
    "\n",
    "# Always unpersist when done\n",
    "analysis_df.unpersist()\n",
    "print(\"\\n‚úì Cache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Caching Strategy Analysis\n",
    "\n",
    "**Task**: Compare performance with and without caching for multiple queries on the same filtered dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Caching Strategy Exercise\n",
    "print(\"üéØ Exercise 2.1: Caching Strategy Analysis\\n\")\n",
    "\n",
    "# Test WITHOUT caching\n",
    "print(\"Scenario 1: Without caching (recompute each time)\")\n",
    "base_df = df.filter(col(\"battery_level\") > 50)\n",
    "\n",
    "start = time.time()\n",
    "query1 = base_df.count()\n",
    "query2 = base_df.agg({'value': 'avg'}).collect()[0][0]\n",
    "query3 = base_df.select('sensor_id').distinct().count()\n",
    "time_without_cache = time.time() - start\n",
    "\n",
    "print(f\"Time: {time_without_cache:.4f}s\")\n",
    "print(f\"Results: {query1:,} records, avg value: {query2:.2f}, {query3} sensors\\n\")\n",
    "\n",
    "# Test WITH caching\n",
    "print(\"Scenario 2: With caching (compute once, reuse)\")\n",
    "cached_base_df = df.filter(col(\"battery_level\") > 50).cache()\n",
    "\n",
    "start = time.time()\n",
    "query1_cached = cached_base_df.count()\n",
    "query2_cached = cached_base_df.agg({'value': 'avg'}).collect()[0][0]\n",
    "query3_cached = cached_base_df.select('sensor_id').distinct().count()\n",
    "time_with_cache = time.time() - start\n",
    "\n",
    "print(f\"Time: {time_with_cache:.4f}s\")\n",
    "print(f\"Results: {query1_cached:,} records, avg value: {query2_cached:.2f}, {query3_cached} sensors\\n\")\n",
    "\n",
    "# Compare\n",
    "print(\"=\"*80)\n",
    "if time_with_cache < time_without_cache:\n",
    "    speedup = time_without_cache / time_with_cache\n",
    "    print(f\"‚úì Caching provided {speedup:.2f}x speedup for multiple queries!\")\n",
    "else:\n",
    "    print(\"Note: Speedup varies based on data size and number of reuses\")\n",
    "\n",
    "# Clean up\n",
    "cached_base_df.unpersist()\n",
    "\n",
    "assert query1 == query1_cached, \"Results should match\"\n",
    "print(\"\\n‚úì Exercise 2.1 completed! Caching benefits multiple operations on same data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Predicate Pushdown with Parquet\n",
    "\n",
    "**Predicate pushdown** is a critical optimization where Spark pushes filter operations down to the data source, reducing the amount of data that needs to be read into memory.\n",
    "\n",
    "### How it Works:\n",
    "- **Parquet format** stores column statistics (min/max values) for each row group\n",
    "- Spark can skip reading entire row groups that don't match filter conditions\n",
    "- This dramatically reduces I/O and improves query performance\n",
    "\n",
    "### What to Look for in Explain Plans:\n",
    "- **`PushedFilters:`** Shows filters pushed to the data source\n",
    "- **`FileScan parquet`** Shows the Parquet scan operation\n",
    "- **No separate `Filter` operation** means filtering happens during read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Parquet data\n",
    "parquet_df = spark.read.parquet(\"../Datasets/iot_sensor_readings.parquet\")\n",
    "\n",
    "print(\"üìä Loaded Parquet dataset\")\n",
    "print(f\"Total records: {parquet_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.1: Parquet Predicate Pushdown\n",
    "\n",
    "Let's apply a filter and examine the explain plan to see predicate pushdown in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Parquet data - observe predicate pushdown\n",
    "filtered_parquet = parquet_df.filter(col(\"status\") == \"NORMAL\")\n",
    "\n",
    "print(\"Parquet with Filter - Explain Plan:\")\n",
    "print(\"=\"*80)\n",
    "filtered_parquet.explain(True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç Look for 'PushedFilters: [IsNotNull(status), EqualTo(status,NORMAL)]' in the scan!\")\n",
    "print(\"This means the filter is applied DURING the read, not after!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.2: CSV vs Parquet Comparison\n",
    "\n",
    "Let's compare Parquet (with pushdown) vs CSV (without pushdown) to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV does NOT support predicate pushdown\n",
    "csv_df = spark.read.csv(\"../Datasets/iot_sensor_readings.csv\", header=True, inferSchema=True)\n",
    "filtered_csv = csv_df.filter(col(\"status\") == \"NORMAL\")\n",
    "\n",
    "print(\"CSV with Filter - Explain Plan:\")\n",
    "print(\"=\"*80)\n",
    "filtered_csv.explain(True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç Notice: No 'PushedFilters' for CSV!\")\n",
    "print(\"CSV must read the entire file, then apply the filter in a separate operation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.3: Performance Comparison\n",
    "\n",
    "Let's measure the actual performance difference between CSV and Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance test: CSV vs Parquet\n",
    "print(\"‚ö° Performance Comparison: CSV vs Parquet\\n\")\n",
    "\n",
    "# Test CSV\n",
    "csv_df_test = spark.read.csv(\"../Datasets/iot_sensor_readings.csv\", header=True, inferSchema=True)\n",
    "start = time.time()\n",
    "csv_filtered = csv_df_test.filter(\n",
    "    (col(\"status\") == \"NORMAL\") & \n",
    "    (col(\"value\") > 50) & \n",
    "    (col(\"battery_level\") > 60)\n",
    ")\n",
    "csv_count = csv_filtered.count()\n",
    "csv_time = time.time() - start\n",
    "\n",
    "# Test Parquet\n",
    "parquet_df_test = spark.read.parquet(\"../Datasets/iot_sensor_readings.parquet\")\n",
    "start = time.time()\n",
    "parquet_filtered = parquet_df_test.filter(\n",
    "    (col(\"status\") == \"NORMAL\") & \n",
    "    (col(\"value\") > 50) & \n",
    "    (col(\"battery_level\") > 60)\n",
    ")\n",
    "parquet_count = parquet_filtered.count()\n",
    "parquet_time = time.time() - start\n",
    "\n",
    "# Results\n",
    "print(f\"CSV:     {csv_count:,} records in {csv_time:.4f}s\")\n",
    "print(f\"Parquet: {parquet_count:,} records in {parquet_time:.4f}s\")\n",
    "print(f\"\\nSpeedup: {csv_time/parquet_time:.2f}x faster with Parquet\")\n",
    "print(f\"Savings: {((csv_time - parquet_time)/csv_time * 100):.1f}% time reduction\")\n",
    "\n",
    "assert csv_count == parquet_count, \"Both should return same results\"\n",
    "print(\"\\n‚úì Results verified - same data, better performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Predicate Pushdown Analysis\n",
    "\n",
    "**Task**: Create a complex filter with multiple conditions and analyze the predicate pushdown behavior for both CSV and Parquet formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Predicate Pushdown Exercise\nprint(\"üéØ Exercise 3.1: Predicate Pushdown Analysis\\n\")\n\n# Complex filter conditions\ncomplex_filter = (\n    (col(\"sensor_id\").startswith(\"TEMP\")) &\n    (col(\"value\") >= 5) &\n    (col(\"value\") <= 10) &\n    (col(\"battery_level\") > 75) &\n    (col(\"location\").contains(\"Building_B\"))\n)\n\n# Test with Parquet\nprint(\"Parquet with complex filter:\")\nparquet_complex = parquet_df.filter(complex_filter)\nparquet_complex.explain()\n\nprint(\"\\n\" + \"=\"*80)\n\n# Test with CSV\nprint(\"CSV with same complex filter:\")\ncsv_complex = csv_df.filter(complex_filter)\ncsv_complex.explain()\n\nprint(\"\\n\" + \"=\"*80)\n\n# Count results\nparquet_result = parquet_complex.count()\ncsv_result = csv_complex.count()\n\nprint(f\"\\nParquet result: {parquet_result:,} records\")\nprint(f\"CSV result: {csv_result:,} records\")\n\nassert parquet_result == csv_result, \"Both should return same results\"\n\nprint(\"\\n‚úì Exercise 3.1 completed!\")\nprint(\"Key insight: Parquet pushed numeric and equality filters to scan, CSV read everything first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Preventing Predicate Pushdown (Anti-Pattern)\n",
    "\n",
    "Caching data **before** filtering prevents predicate pushdown optimization. This is a common anti-pattern to avoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.1: The Anti-Pattern\n",
    "\n",
    "Let's see what happens when we cache before filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anti-pattern: Cache BEFORE filtering\n",
    "print(\"‚ùå Anti-Pattern: Caching before filtering\\n\")\n",
    "\n",
    "# Load and cache immediately (before any filters)\n",
    "cached_parquet = spark.read.parquet(\"../Datasets/iot_sensor_readings.parquet\").cache()\n",
    "\n",
    "# Force caching with an action\n",
    "cached_count = cached_parquet.count()\n",
    "print(f\"Cached {cached_count:,} records (entire dataset)\\n\")\n",
    "\n",
    "# Now filter the cached data\n",
    "filtered_from_cache = cached_parquet.filter(col(\"status\") == \"NORMAL\")\n",
    "\n",
    "print(\"Explain plan for filter on cached data:\")\n",
    "print(\"=\"*80)\n",
    "filtered_from_cache.explain(True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç Look for 'InMemoryTableScan' followed by 'Filter'!\")\n",
    "print(\"This means Spark had to:\")\n",
    "print(\"  1. Read the ENTIRE Parquet file\")\n",
    "print(\"  2. Cache ALL the data in memory\")\n",
    "print(\"  3. Scan the cached data and apply the filter\")\n",
    "print(\"\\nPredicate pushdown is lost!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.2: The Correct Pattern\n",
    "\n",
    "If you must cache, filter FIRST to cache only what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct pattern: Filter BEFORE caching\n",
    "print(\"‚úì Correct Pattern: Filtering before caching\\n\")\n",
    "\n",
    "# Load, filter, then cache\n",
    "parquet_filtered_first = (\n",
    "    spark.read.parquet(\"../Datasets/iot_sensor_readings.parquet\")\n",
    "    .filter(col(\"status\") == \"NORMAL\")  # Filter first!\n",
    "    .cache()  # Then cache the smaller result\n",
    ")\n",
    "\n",
    "filtered_count = parquet_filtered_first.count()\n",
    "print(f\"Cached only {filtered_count:,} filtered records (smaller dataset)\\n\")\n",
    "\n",
    "print(\"Benefits:\")\n",
    "print(\"  1. Predicate pushdown worked during initial read\")\n",
    "print(\"  2. Less memory used (only caching filtered data)\")\n",
    "print(\"  3. Faster caching (less data to materialize)\")\n",
    "print(f\"\\nMemory savings: {((cached_count - filtered_count) / cached_count * 100):.1f}% less data cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.3: Performance Impact\n",
    "\n",
    "Let's measure the performance difference between caching before vs after filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: Cache before vs after filtering\n",
    "print(\"‚ö° Performance Impact: Cache Before vs After Filtering\\n\")\n",
    "\n",
    "# Clear any existing cache\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Scenario 1: Cache before filtering (anti-pattern)\n",
    "start = time.time()\n",
    "df_cache_first = spark.read.parquet(\"../Datasets/iot_sensor_readings.parquet\").cache()\n",
    "df_cache_first.count()  # Materialize cache\n",
    "result1 = df_cache_first.filter(\n",
    "    (col(\"status\") == \"NORMAL\") & (col(\"value\") > 50)\n",
    ").count()\n",
    "time_cache_before = time.time() - start\n",
    "\n",
    "print(f\"Cache before filtering: {result1:,} results in {time_cache_before:.4f}s\")\n",
    "\n",
    "# Clear cache\n",
    "df_cache_first.unpersist()\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Scenario 2: Filter before caching (correct pattern)\n",
    "start = time.time()\n",
    "df_filter_first = (\n",
    "    spark.read.parquet(\"../Datasets/iot_sensor_readings.parquet\")\n",
    "    .filter((col(\"status\") == \"NORMAL\") & (col(\"value\") > 50))\n",
    "    .cache()\n",
    ")\n",
    "result2 = df_filter_first.count()  # Materialize cache\n",
    "time_filter_before = time.time() - start\n",
    "\n",
    "print(f\"Filter before caching: {result2:,} results in {time_filter_before:.4f}s\")\n",
    "\n",
    "# Cleanup\n",
    "df_filter_first.unpersist()\n",
    "cached_parquet.unpersist()\n",
    "parquet_filtered_first.unpersist()\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "# Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if time_filter_before < time_cache_before:\n",
    "    improvement = ((time_cache_before - time_filter_before) / time_cache_before * 100)\n",
    "    print(f\"‚úì Filtering first is {improvement:.1f}% faster!\")\n",
    "    print(f\"Speedup: {time_cache_before/time_filter_before:.2f}x\")\n",
    "else:\n",
    "    print(\"Note: Performance varies by data size and filter selectivity\")\n",
    "\n",
    "assert result1 == result2, \"Both approaches should return same results\"\n",
    "print(\"\\n‚úì Performance comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Identify Anti-Patterns\n",
    "\n",
    "**Task**: Review the explain plans for cached vs uncached filtered queries and identify the performance implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Anti-Pattern Identification Exercise\nprint(\"üéØ Exercise 4.1: Identify Anti-Patterns\\n\")\n\n# Setup test scenarios\nspark.catalog.clearCache()\n\n# Scenario A: No caching (baseline with predicate pushdown)\nscenario_a = (\n    spark.read.parquet(\"../Datasets/iot_sensor_readings.parquet\")\n    .filter((col(\"sensor_id\").startswith(\"HUMID\")) & (col(\"value\") > 40))\n)\n\nprint(\"Scenario A: No caching (predicate pushdown enabled)\")\nscenario_a.explain()\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# Scenario B: Cache full dataset then filter (anti-pattern)\nbase_cached = spark.read.parquet(\"../Datasets/iot_sensor_readings.parquet\").cache()\nbase_cached.count()  # Materialize\nscenario_b = base_cached.filter(\n    (col(\"sensor_id\").startswith(\"HUMID\")) & (col(\"value\") > 40)\n)\n\nprint(\"Scenario B: Cache before filter (anti-pattern)\")\nscenario_b.explain()\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# Scenario C: Filter then cache (correct if multiple reuses needed)\nscenario_c = (\n    spark.read.parquet(\"../Datasets/iot_sensor_readings.parquet\")\n    .filter((col(\"sensor_id\").startswith(\"HUMID\")) & (col(\"value\") > 40))\n    .cache()\n)\nscenario_c.count()  # Materialize\n\nprint(\"Scenario C: Filter before cache (correct pattern)\")\nscenario_c.explain()\nprint(\"\\n\" + \"=\"*80 + \"\\n\")\n\n# Verify results\ncount_a = scenario_a.count()\ncount_b = scenario_b.count()\ncount_c = scenario_c.count()\n\nprint(f\"Results:\")\nprint(f\"  Scenario A (no cache): {count_a:,} records\")\nprint(f\"  Scenario B (cache before): {count_b:,} records\")\nprint(f\"  Scenario C (filter before cache): {count_c:,} records\")\n\n# Cleanup\nbase_cached.unpersist()\nscenario_c.unpersist()\nspark.catalog.clearCache()\n\nassert count_a == count_b == count_c, \"All scenarios should return same results\"\n\nprint(\"\\n‚úì Exercise 4.1 completed!\")\nprint(\"\\nKey Takeaways:\")\nprint(\"  ‚úì Scenario A: Best for single-use queries (predicate pushdown)\")\nprint(\"  ‚ùå Scenario B: Anti-pattern (loses predicate pushdown, wastes memory)\")\nprint(\"  ‚úì Scenario C: Best for multiple reuses (predicate pushdown + caching only what's needed)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Query Optimization Best Practices\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "\n",
    "1. **Catalyst Optimizer**\n",
    "   - Automatically consolidates multiple filters\n",
    "   - Eliminates duplicate conditions\n",
    "   - Write for readability - Catalyst handles efficiency\n",
    "\n",
    "2. **Caching Strategy**\n",
    "   - ‚úÖ Cache for iterative operations (EDA, ML training)\n",
    "   - ‚ùå Don't cache for single-use queries\n",
    "   - ‚úÖ Always `unpersist()` when done\n",
    "   - ‚úÖ Filter BEFORE caching to reduce memory usage\n",
    "\n",
    "3. **Predicate Pushdown**\n",
    "   - Parquet enables pushdown via column statistics\n",
    "   - Look for `PushedFilters` in explain plans\n",
    "   - 2-5x performance improvement typical\n",
    "   - CSV requires full file scan, then filtering\n",
    "\n",
    "4. **Anti-Patterns to Avoid**\n",
    "   - ‚ùå Caching before filtering (loses predicate pushdown)\n",
    "   - ‚ùå Using CSV for large analytical queries (use Parquet)\n",
    "   - ‚ùå Caching rarely-used data (wastes resources)\n",
    "\n",
    "### Performance Impact Summary:\n",
    "\n",
    "| **Optimization** | **Impact** | **When to Use** |\n",
    "|-----------------|-----------|----------------|\n",
    "| Parquet vs CSV | 2-5x faster | Always for analytics |\n",
    "| Predicate Pushdown | 3-10x faster | Filter on indexed columns |\n",
    "| Caching (correct) | 2-3x faster | Multiple operations on same data |\n",
    "| Filter before Cache | 50-80% memory savings | When caching is necessary |\n",
    "\n",
    "### Best Practices:\n",
    "1. ‚úÖ Use Parquet for analytical workloads\n",
    "2. ‚úÖ Apply filters as early as possible\n",
    "3. ‚úÖ Let Catalyst optimize - write readable code\n",
    "4. ‚úÖ Cache only when reusing data multiple times\n",
    "5. ‚úÖ Filter before caching\n",
    "6. ‚úÖ Always unpersist() cached DataFrames\n",
    "7. ‚úÖ Use `explain()` to verify optimizations\n",
    "8. ‚úÖ Monitor Spark UI for query performance\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: Apply these optimization techniques to your data pipelines. Use `explain()` regularly to verify that Spark is optimizing your queries as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "spark.catalog.clearCache()\n",
    "spark.stop()\n",
    "\n",
    "print(\"üéâ Lab: Query Optimization completed!\")\n",
    "print(\"\\n‚úì Learned: Catalyst optimization, caching strategies, predicate pushdown, and anti-patterns\")\n",
    "print(\"\\n‚û°Ô∏è  Next: Apply these techniques to optimize your Spark applications!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}