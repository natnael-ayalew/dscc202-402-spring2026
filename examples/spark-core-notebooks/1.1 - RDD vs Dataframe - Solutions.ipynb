{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD vs DataFrame: Performance & API Comparison\n",
    "\n",
    "This notebook demonstrates the fundamental differences between RDDs and DataFrames using word count on the **Complete Works of Shakespeare**.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Aspect | RDD | DataFrame |\n",
    "|--------|-----|------------|\n",
    "| **Paradigm** | Imperative (HOW) | Declarative (WHAT) |\n",
    "| **Optimization** | None - executes as written | Catalyst optimizer |\n",
    "| **Execution** | Java object serialization | Tungsten + code generation |\n",
    "| **Type Safety** | Compile-time (Scala) | Runtime (schema) |\n",
    "| **Best For** | Low-level control, unstructured data | Structured/semi-structured data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    explode, split, lower, col, regexp_replace, trim, length\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Shakespeare text\n",
    "FILEPATH = \"shakespeare.txt\"\n",
    "URL = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
    "\n",
    "if not Path(FILEPATH).exists():\n",
    "    print(f\"Downloading from {URL}...\")\n",
    "    urllib.request.urlretrieve(URL, FILEPATH)\n",
    "\n",
    "file_size = Path(FILEPATH).stat().st_size\n",
    "print(f\"File size: {file_size / 1024:.1f} KB ({file_size / 1024 / 1024:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD_vs_DataFrame_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Running on: {sc.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## RDD Implementation\n",
    "\n",
    "With RDDs, you specify **HOW** to process data step by step. Spark executes exactly what you write with no optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_rdd(filepath: str):\n",
    "    \"\"\"\n",
    "    Classic RDD word count - imperative style.\n",
    "    \n",
    "    Step by step:\n",
    "    1. textFile()     -> Read lines from file\n",
    "    2. flatMap()      -> Split lines into words (one row per word)\n",
    "    3. map()          -> Normalize each word\n",
    "    4. filter()       -> Remove empty strings\n",
    "    5. map()          -> Create (word, 1) pairs\n",
    "    6. reduceByKey()  -> Sum counts per word\n",
    "    7. sortBy()       -> Order by count\n",
    "    \"\"\"\n",
    "    return (\n",
    "        sc.textFile(filepath)\n",
    "        .flatMap(lambda line: line.split())\n",
    "        .map(lambda word: ''.join(c for c in word.lower() if c.isalpha()))\n",
    "        .filter(lambda word: len(word) > 0)\n",
    "        .map(lambda word: (word, 1))\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        .sortBy(lambda x: x[1], ascending=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute and time\n",
    "start = time.perf_counter()\n",
    "rdd_result = word_count_rdd(FILEPATH).collect()\n",
    "rdd_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"RDD execution time: {rdd_time:.3f}s\")\n",
    "print(f\"\\nTop 10 words:\")\n",
    "for word, count in rdd_result[:10]:\n",
    "    print(f\"  {word:15} {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DataFrame Implementation\n",
    "\n",
    "With DataFrames, you specify **WHAT** you want. The Catalyst optimizer decides **HOW** to execute efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_dataframe(filepath: str):\n",
    "    \"\"\"\n",
    "    DataFrame word count - declarative style.\n",
    "    \n",
    "    Benefits:\n",
    "    - Catalyst optimizer rewrites query for efficiency\n",
    "    - Tungsten engine: off-heap memory, code generation\n",
    "    - Automatic predicate pushdown, column pruning\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read.text(filepath)\n",
    "        # Split into words and explode to rows\n",
    "        .select(explode(split(col(\"value\"), r\"\\s+\")).alias(\"word\"))\n",
    "        # Normalize: lowercase, keep only letters\n",
    "        .select(regexp_replace(lower(col(\"word\")), r\"[^a-z]\", \"\").alias(\"word\"))\n",
    "        # Filter empty\n",
    "        .filter(length(col(\"word\")) > 0)\n",
    "        # Group and count\n",
    "        .groupBy(\"word\").count()\n",
    "        # Sort\n",
    "        .orderBy(col(\"count\").desc())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute and time\n",
    "start = time.perf_counter()\n",
    "df_result = word_count_dataframe(FILEPATH).collect()\n",
    "df_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"DataFrame execution time: {df_time:.3f}s\")\n",
    "print(f\"\\nTop 10 words:\")\n",
    "for row in df_result[:10]:\n",
    "    print(f\"  {row['word']:15} {row['count']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“¦ Understanding Partitions\n",
    "\n",
    "Partitions are the fundamental unit of parallelism in Spark. Each partition is processed by one task on one core. Understanding partitions helps you understand:\n",
    "\n",
    "- **Why shuffles are expensive** â€” data must move between partitions across the network\n",
    "- **How parallelism works** â€” more partitions = more parallel tasks (up to available cores)\n",
    "- **What the execution plan means** â€” `hashpartitioning(word, 8)` means 8 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check partition counts at different stages\n",
    "raw_rdd = sc.textFile(FILEPATH)\n",
    "words_rdd = raw_rdd.flatMap(lambda line: line.split())\n",
    "pairs_rdd = words_rdd.map(lambda w: (w.lower(), 1))\n",
    "counts_rdd = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"RDD Partition Counts at Each Stage:\")\n",
    "print(f\"  After textFile():    {raw_rdd.getNumPartitions()} partitions\")\n",
    "print(f\"  After flatMap():     {words_rdd.getNumPartitions()} partitions (narrow transform - no change)\")\n",
    "print(f\"  After map():         {pairs_rdd.getNumPartitions()} partitions (narrow transform - no change)\")\n",
    "print(f\"  After reduceByKey(): {counts_rdd.getNumPartitions()} partitions (shuffle may change this)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame partition count\n",
    "raw_df = spark.read.text(FILEPATH)\n",
    "print(f\"\\nDataFrame Partitions: {raw_df.rdd.getNumPartitions()}\")\n",
    "print(f\"Shuffle partitions (spark.sql.shuffle.partitions): {spark.conf.get('spark.sql.shuffle.partitions')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Data Distribution Across Partitions\n",
    "\n",
    "Let's see how words are distributed across partitions **before** and **after** the shuffle. This illustrates why shuffles are necessary for aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glom() to see data grouped by partition\n",
    "# glom() collects all elements in each partition into a list\n",
    "\n",
    "# Sample: first 5 words from each partition BEFORE shuffle\n",
    "pairs_sample = pairs_rdd.mapPartitionsWithIndex(\n",
    "    lambda idx, it: [(idx, list(itertools.islice(it, 5)))]\n",
    ").collect()\n",
    "\n",
    "print(\"BEFORE SHUFFLE (reduceByKey): Words are scattered randomly\")\n",
    "print(\"=\" * 60)\n",
    "for partition_id, words in pairs_sample[:4]:  # Show first 4 partitions\n",
    "    word_list = [w[0] for w in words]\n",
    "    print(f\"Partition {partition_id}: {word_list}\")\n",
    "\n",
    "print(\"\\nâ†’ Notice: the same word can appear in multiple partitions!\")\n",
    "print(\"  This is why we need a shuffle to group by key.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample: first 5 words from each partition AFTER shuffle\n",
    "counts_sample = counts_rdd.mapPartitionsWithIndex(\n",
    "    lambda idx, it: [(idx, list(itertools.islice(it, 5)))]\n",
    ").collect()\n",
    "\n",
    "print(\"AFTER SHUFFLE (reduceByKey): Each word lives in exactly ONE partition\")\n",
    "print(\"=\" * 60)\n",
    "for partition_id, words in counts_sample[:4]:\n",
    "    word_list = [f\"{w[0]}({w[1]})\" for w in words]\n",
    "    print(f\"Partition {partition_id}: {word_list}\")\n",
    "\n",
    "print(\"\\nâ†’ Now each word appears in only one partition with its total count.\")\n",
    "print(\"  The shuffle moved all occurrences of each word to the same partition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize partition sizes\n",
    "partition_sizes_before = pairs_rdd.glom().map(len).collect()\n",
    "partition_sizes_after = counts_rdd.glom().map(len).collect()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Before shuffle\n",
    "ax1 = axes[0]\n",
    "ax1.bar(range(len(partition_sizes_before)), partition_sizes_before, color='#e74c3c', edgecolor='black')\n",
    "ax1.set_xlabel('Partition ID')\n",
    "ax1.set_ylabel('Number of Records')\n",
    "ax1.set_title('BEFORE Shuffle: (word, 1) pairs per partition', fontweight='bold')\n",
    "ax1.set_xticks(range(len(partition_sizes_before)))\n",
    "\n",
    "# After shuffle\n",
    "ax2 = axes[1]\n",
    "ax2.bar(range(len(partition_sizes_after)), partition_sizes_after, color='#3498db', edgecolor='black')\n",
    "ax2.set_xlabel('Partition ID')\n",
    "ax2.set_ylabel('Number of Records')\n",
    "ax2.set_title('AFTER Shuffle: (word, count) pairs per partition', fontweight='bold')\n",
    "ax2.set_xticks(range(len(partition_sizes_after)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('partition_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal records BEFORE shuffle: {sum(partition_sizes_before):,} (every word occurrence)\")\n",
    "print(f\"Total records AFTER shuffle:  {sum(partition_sizes_after):,} (unique words only)\")\n",
    "print(f\"\\nâ†’ The shuffle dramatically reduced data volume by pre-aggregating!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Hash Partitioning Works\n",
    "\n",
    "After `reduceByKey`, words are distributed using **hash partitioning**:\n",
    "\n",
    "```\n",
    "partition_id = hash(word) % num_partitions\n",
    "```\n",
    "\n",
    "This ensures:\n",
    "1. **All occurrences of the same word go to the same partition** â€” required for correct aggregation\n",
    "2. **Deterministic assignment** â€” same word always maps to same partition\n",
    "3. **Roughly even distribution** â€” hash functions spread keys uniformly (ideally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate hash partitioning\n",
    "num_partitions = counts_rdd.getNumPartitions()\n",
    "sample_words = ['the', 'and', 'of', 'to', 'a', 'hamlet', 'romeo', 'juliet']\n",
    "\n",
    "print(f\"Hash partitioning with {num_partitions} partitions:\")\n",
    "print(\"=\" * 40)\n",
    "for word in sample_words:\n",
    "    partition = hash(word) % num_partitions\n",
    "    print(f\"  '{word}' â†’ partition {partition}\")\n",
    "\n",
    "print(\"\\nâ†’ This is why 'hashpartitioning(word, 8)' appears in the execution plan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrow vs Wide Transformations\n",
    "\n",
    "| Type | Examples | Shuffle? | Partition Change? |\n",
    "|------|----------|----------|-------------------|\n",
    "| **Narrow** | `map`, `filter`, `flatMap` | No | Data stays in same partition |\n",
    "| **Wide** | `reduceByKey`, `groupByKey`, `join`, `sort` | Yes | Data moves between partitions |\n",
    "\n",
    "**Rule of thumb**: If an operation needs to see data from multiple partitions to produce a result, it requires a shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ” The Key Insight: Query Execution Plans\n",
    "\n",
    "This is where the real difference becomes visible. The DataFrame's `explain()` shows how Catalyst transforms your query through multiple stages of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query (lazy - not executed yet)\n",
    "df_query = word_count_dataframe(FILEPATH)\n",
    "\n",
    "# Show the full execution plan\n",
    "print(\"=\" * 70)\n",
    "print(\"DATAFRAME EXECUTION PLAN\")\n",
    "print(\"=\" * 70)\n",
    "df_query.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the DataFrame Physical Execution Plan\n",
    "\n",
    "This plan shows **exactly how Spark will execute your DataFrame query**. Read it **bottom-up** â€” data flows from step (1) to step (10).\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Breakdown\n",
    "\n",
    "**`(1) Scan text`** â€” Reads the raw text file. Each line becomes one row with a single column called `value`. The `ReadSchema` confirms we're only reading what we need (column pruning).\n",
    "\n",
    "**`(2) Generate`** â€” The `explode(split(...))` operation. Splits each line on whitespace and \"explodes\" the resulting array so each word becomes its own row. One line with 10 words â†’ 10 rows.\n",
    "\n",
    "**`(3) Filter`** â€” Removes empty strings. Notice the condition combines `lower()`, `regexp_replace()`, and `length()` â€” Catalyst **fused these operations** rather than doing three separate passes.\n",
    "\n",
    "**`(4) Project`** â€” Applies the lowercase + regex cleanup transformation. This is the normalized word output.\n",
    "\n",
    "**`(5) HashAggregate` (partial)** â€” This is the first half of a **two-phase aggregation**. Each partition computes local counts *before* shuffling. The `partial_count` means \"count what I have locally.\" This is a major optimization â€” instead of shuffling every word, we shuffle pre-aggregated `(word, partial_count)` pairs.\n",
    "\n",
    "**`(6) Exchange`** â€” âš ï¸ **SHUFFLE**. Data is redistributed across the cluster using `hashpartitioning(word, 8)`. All occurrences of the same word end up on the same partition. Shuffles are expensive (network I/O), so Spark minimizes them.\n",
    "\n",
    "**`(7) HashAggregate` (final)** â€” Combines the partial counts from all partitions into final counts per word.\n",
    "\n",
    "**`(8) Exchange`** â€” Another shuffle, this time using `rangepartitioning` to prepare for global sorting. Data is distributed so partition 1 has the highest counts, partition 2 the next highest, etc.\n",
    "\n",
    "**`(9) Sort`** â€” Sorts within each partition. Because of the range partitioning, the final result is globally sorted.\n",
    "\n",
    "**`(10) AdaptiveSparkPlan`** â€” Wrapper indicating **Adaptive Query Execution (AQE)** is enabled. Spark can modify this plan at runtime based on actual data statistics (e.g., adjusting partition sizes after seeing shuffle data).\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Concepts Illustrated\n",
    "\n",
    "| Concept | Where You See It |\n",
    "|---------|------------------|\n",
    "| **Two-phase aggregation** | Steps 5 â†’ 6 â†’ 7 (partial agg, shuffle, final agg) |\n",
    "| **Predicate pushdown / fusion** | Step 3 combines multiple operations |\n",
    "| **Shuffle = Exchange** | Steps 6 and 8 â€” the expensive operations |\n",
    "| **Adaptive Query Execution** | Step 10 wrapper |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDDs have no equivalent - you can only see the DAG lineage\n",
    "rdd_query = word_count_rdd(FILEPATH)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RDD DEBUG STRING (Lineage only - no optimization info)\")\n",
    "print(\"=\" * 70)\n",
    "print(rdd_query.toDebugString().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the RDD Lineage (Debug String)\n",
    "\n",
    "This shows the **DAG (Directed Acyclic Graph)** of RDD transformations â€” essentially a record of *what you wrote*, not an optimized execution plan. Read it **bottom-up**, following the indentation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Breakdown\n",
    "\n",
    "**`HadoopRDD` â†’ `MapPartitionsRDD`** â€” `sc.textFile()` reads the file. HadoopRDD handles the raw file read, then it's immediately wrapped in a MapPartitionsRDD to produce string lines.\n",
    "\n",
    "**`PythonRDD`** â€” Your `flatMap()`, `map()`, `filter()`, and second `map()` are all here. PySpark batches consecutive narrow transformations into a single Python worker call.\n",
    "\n",
    "**`PairwiseRDD`** â€” Preparation for `reduceByKey()`. Converts the Python `(word, 1)` tuples into a format Spark can shuffle.\n",
    "\n",
    "**`ShuffledRDD`** â€” âš ï¸ **SHUFFLE #1** for `reduceByKey()`. All identical keys are gathered to the same partition.\n",
    "\n",
    "**`MapPartitionsRDD` â†’ `PythonRDD`** â€” Post-shuffle processing and setup for sorting.\n",
    "\n",
    "**`PairwiseRDD`** â€” Preparation for `sortBy()`. Samples data to determine range boundaries.\n",
    "\n",
    "**`ShuffledRDD`** â€” âš ï¸ **SHUFFLE #2** for `sortBy()`. Range partitioning to achieve global sort order.\n",
    "\n",
    "**`MapPartitionsRDD` â†’ `PythonRDD`** â€” Final sort within each partition and conversion back to Python objects.\n",
    "\n",
    "---\n",
    "\n",
    "#### What's Missing (Compared to DataFrame)\n",
    "\n",
    "| Aspect | DataFrame Plan | RDD Lineage |\n",
    "|--------|----------------|-------------|\n",
    "| **Optimization details** | Shows predicate fusion, two-phase aggregation | Just shows \"PythonRDD\" â€” no insight into what's inside |\n",
    "| **Column pruning** | Explicit `ReadSchema` | N/A (no schema) |\n",
    "| **Aggregation strategy** | Clearly shows partial â†’ shuffle â†’ final | Hidden inside opaque Python calls |\n",
    "| **Operation fusion** | Visible in plan | Happens implicitly, not shown |\n",
    "| **Adaptive execution** | AQE wrapper present | Not available |\n",
    "\n",
    "---\n",
    "\n",
    "#### The Key Insight\n",
    "\n",
    "Notice how your Python transformations collapse into opaque `PythonRDD` blocks. Spark can't see inside them â€” it just knows \"call this Python function.\" This means:\n",
    "\n",
    "1. **No Catalyst optimization** â€” Spark executes exactly what you wrote\n",
    "2. **No Tungsten benefits** â€” Data serializes between JVM and Python workers\n",
    "3. **No introspection** â€” You can't see if your code is efficient\n",
    "\n",
    "The `(2)` at the start of each line indicates **2 partitions**. That's about all the runtime insight you get.\n",
    "\n",
    "---\n",
    "\n",
    "#### When This Matters\n",
    "\n",
    "For simple jobs, the difference is minor. But imagine you wrote an inefficient filter after a join â€” DataFrame's Catalyst would push the filter before the join automatically. With RDDs, you'd get exactly the inefficient plan you wrote, and this debug string wouldn't even show you that it's a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Benchmarking: Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(name: str, func, n_runs: int = 5, warmup: int = 1):\n",
    "    \"\"\"Run benchmark with warmup.\"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        func()\n",
    "    \n",
    "    # Timed runs\n",
    "    times = []\n",
    "    for i in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        result = func()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"times\": times,\n",
    "        \"mean\": np.mean(times),\n",
    "        \"std\": np.std(times),\n",
    "        \"min\": np.min(times),\n",
    "        \"max\": np.max(times)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUNS = 5\n",
    "\n",
    "print(f\"Running {N_RUNS} benchmark iterations (plus warmup)...\\n\")\n",
    "\n",
    "rdd_bench = benchmark(\n",
    "    \"RDD\", \n",
    "    lambda: word_count_rdd(FILEPATH).collect(),\n",
    "    n_runs=N_RUNS\n",
    ")\n",
    "print(f\"RDD:       {rdd_bench['mean']:.3f}s Â± {rdd_bench['std']:.3f}s\")\n",
    "\n",
    "df_bench = benchmark(\n",
    "    \"DataFrame\",\n",
    "    lambda: word_count_dataframe(FILEPATH).collect(),\n",
    "    n_runs=N_RUNS\n",
    ")\n",
    "print(f\"DataFrame: {df_bench['mean']:.3f}s Â± {df_bench['std']:.3f}s\")\n",
    "\n",
    "speedup = rdd_bench['mean'] / df_bench['mean']\n",
    "print(f\"\\nðŸš€ DataFrame is {speedup:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- Performance Comparison ---\n",
    "ax1 = axes[0]\n",
    "names = [\"RDD\", \"DataFrame\"]\n",
    "means = [rdd_bench[\"mean\"], df_bench[\"mean\"]]\n",
    "stds = [rdd_bench[\"std\"], df_bench[\"std\"]]\n",
    "colors = [\"#e74c3c\", \"#3498db\"]\n",
    "\n",
    "bars = ax1.bar(names, means, yerr=stds, capsize=8, color=colors, \n",
    "               edgecolor=\"black\", linewidth=1.5)\n",
    "\n",
    "ax1.set_ylabel(\"Execution Time (seconds)\", fontsize=12)\n",
    "ax1.set_title(\"Word Count Performance: RDD vs DataFrame\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean, std in zip(bars, means, stds):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.05,\n",
    "            f\"{mean:.3f}s\", ha=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "# Add speedup annotation\n",
    "ax1.annotate(f\"{speedup:.1f}x faster\", \n",
    "            xy=(1, df_bench[\"mean\"]), \n",
    "            xytext=(0.5, (rdd_bench[\"mean\"] + df_bench[\"mean\"])/2),\n",
    "            fontsize=14, fontweight=\"bold\", color=\"#27ae60\",\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"#27ae60\", lw=2))\n",
    "\n",
    "# --- Top Words ---\n",
    "ax2 = axes[1]\n",
    "top_words = df_result[:15]\n",
    "words = [row[\"word\"] for row in top_words]\n",
    "counts = [row[\"count\"] for row in top_words]\n",
    "\n",
    "y_pos = np.arange(len(words))\n",
    "ax2.barh(y_pos, counts, color=\"#3498db\", edgecolor=\"black\", linewidth=0.5)\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(words, fontsize=11)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_xlabel(\"Frequency\", fontsize=12)\n",
    "ax2.set_title(\"Top 15 Words in Shakespeare\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "for i, count in enumerate(counts):\n",
    "    ax2.text(count + 200, i, f\"{count:,}\", va=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"benchmark_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Why DataFrames Are Faster\n",
    "\n",
    "### 1. Catalyst Optimizer\n",
    "- **Predicate pushdown**: Filters moved closer to data source\n",
    "- **Column pruning**: Only reads columns actually needed\n",
    "- **Constant folding**: Evaluates constant expressions at compile time\n",
    "- **Join reordering**: Optimizes join order based on statistics\n",
    "\n",
    "### 2. Tungsten Execution Engine\n",
    "- **Off-heap memory**: Avoids JVM garbage collection overhead\n",
    "- **Cache-aware computation**: Optimized for CPU cache hierarchy\n",
    "- **Whole-stage code generation**: Compiles query stages to optimized bytecode\n",
    "\n",
    "### 3. Efficient Data Representation\n",
    "- RDDs: Java objects with full serialization overhead\n",
    "- DataFrames: Binary row format, compact and efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## When to Use RDDs\n",
    "\n",
    "RDDs still have valid use cases:\n",
    "\n",
    "1. **Unstructured data** that doesn't fit a columnar schema\n",
    "2. **Low-level transformations** not expressible in DataFrame API\n",
    "3. **Fine-grained partitioning control** for specific performance tuning\n",
    "4. **Legacy code** that predates DataFrame API\n",
    "5. **Type safety in Scala** with compile-time checking\n",
    "\n",
    "For most structured data processing: **prefer DataFrames**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
